---
title: Team Project Deliverable 2
author:  | 
  | Joan Cortes - 100438579
  | Jose Antonio Jijon Vorbeck - 100438045
  | Didier Dirks - 100443386
date: "22-01-2021"
indent: false
output:
  html_document: default
  pdf_document: default
---

```{r Import_data, include=FALSE}
# Imports
lapply(c("naniar", "ggplot2", "mice", "MASS", "corrplot", "plotrix", "purrr", "tidyr", "ggplot2", "ica", "factoextra", "cluster", "pracma", "mclust", "moments", "naivebayes", "RSpectra", "corpcor", "PRIMME", "dplyr", "nnet", "class"), require, character.only = TRUE)

#Instantiate colors
color_1 <- "deepskyblue2"
color_2 <- "seagreen2"
color_3 <- "orange2"
color_4 <- "darkorchid4"
color_5 <- "orchid2"
color_6 <- "chocolate4"

# Load data 
# put your directory here
# setwd("/Users/joseantoniojijon/Documents/UC3M/Clases/Bimestre_II/StatisticalLearning/TeamProject/statisticallearning/Part2")
# load('transformed_data.RData')

load('reduced_X.RData')
X = small_X[,1:24]

```


This report will be divided into two main sections. The first part will be devoted to unsupervised learning using several methodologies to make clusters learned in class. The second part of the report will be devoted to a supervised learning using methods like KNN, bayes theorems and logistic regressions.

The data that we will be using throughout the analysis is the same as the one for the previous step of the project, it consist of AirBnB listings along with many characteristics about their location, owner info and services included. In the first part of the project we have already done the preprocessing of the data (imputation of missing data, selection of meaningful attributes, removal of non realistic data etc.). We will take this already modified data and use it for the rest of the project. 

For computation purposes, we had to take only a smaller part of the original data, since it contained more than 20k observations, and our personal computers where taking more than 30 mins to compute the silhouette of each of the examples. With a smaller dataset, we kept the proportions of the groups, and tried to stay with the same overall characteristics, by splitting the data randomly.

The categorical variable of interest that we have chosen for making the true clusters is "room_type". This variable contains 4 classes, which are: 'Entire home/apt', 'Hotel room', 'Private Room' and 'Shared room'. Most of the instances are divided into entire homes or private rooms, and the other two categories combined add up to only 3.5% of the whole dataset. 

We have performed the principal component analysis for the selected dataset, and plotted the first two principal components in terms of room_type. Below we see the graph.

```{r visual_analysis, echo=FALSE}
# PCA for the data set to try to see if the groups are better seen in the PCS
X_pcs <- prcomp(X,scale=TRUE)

# Make a plot of the first two PCs by coloring the room_type
colpal = seq(length(small_X$room_type))
colpal[which(small_X$room_type=="Entire home/apt")] = color_1
colpal[which(small_X$room_type=="Hotel room")] = color_2
colpal[which(small_X$room_type=="Private room")] = color_3
colpal[which(small_X$room_type=="Shared room")] = color_4

plot(X_pcs$x[,1:2],pch=20,col=colpal,cex.lab=1, cex.axis=1, cex.main=1, cex.sub=1, cex.main = 1,main="First two PCs for the Data, in terms of room type")
legend("topleft", legend=c("Entire home/apt", "Hotel room", "Private room", "Shared room"), col=c(color_1, color_2, color_3, color_4),pch=19, cex=0.5)
```

We can see that ther is not a very strong separation when we choose the grouping criterion to be the room_type, maybe because there are many other characteristics that will have a greater impact on the separation of the groups. This is nevertheless very interesting, since we will see, throught the report, how the different algorithms and methods chosen will make different groups out of the same data, according to its different criterions.

## 1. Cluster analysis (unsupervised classification). 

**For that you have to skip the categorical variable of interest for carrying out supervised classification. It is important to note that the groups obtained with the clustering may not be those that define the categorical variable of interest. Obtain conclusions from the analysis.**


The unsupervised cluster analysis can be done with several methodologies. In this report we will present the following ones:

* Partitional Clustering
  - K-means
  - K-medoids
  
* Hierarchical Clustering
  - Agglomerative
  - Divisive
  
* Model Based Clustering

### Partitional Clustering 

Clustering  refers to a wide range of methods for finding subgroups (clusters) in a dataset. 
The aim of these techniques is to group observations so the instances assigned to a particular group are quite homogenous, whereas the observations in the other groups are as different as possible from each other.   

In this section we are going to focus on the family of methods belonging to Partitional clustering. The idea of Partitional clustering is to start with a particular partition and exchange observations until an optimal clustering structure is reached. We are going to employ the most standard algorithms belonging to Partitional clustering such us K-means, PAM, and CLARA.

We start by assessing the existence of subgroups in our dataset with the use of the K-means algorithm. For every specific number of groups selected, the algorithm will try to minimize the within-cluster variation. This is to say, the difference between each observation belonging to a specific group and the sample mean associated to that specific group. The algorithm then stops when stabilizes. 

#### K-means
```{r kmeans, echo = FALSE}
y <- small_X["room_type"]


small_X_transf <- small_X%>%
  mutate(Centro= ifelse(small_X$Centro == "1", "Yes", "No"),
         host_is_superhost= ifelse(small_X$host_is_superhost == "f", "No", "Yes")) %>%
  select(-c("room_type",  "neighbourhood_cleansed"))

small_X_complete <- small_X_transf
small_X_transf <- small_X_transf[,1:24]

#getting rid off the response variable
small_X_qda <- small_X%>%
  mutate(Centro= ifelse(small_X$Centro == "1", "Yes", "No"),
         host_is_superhost= ifelse(small_X$host_is_superhost == "f", "No", "Yes"))%>%
  filter(small_X$room_type == "Entire home/apt"| small_X$room_type == "Private room")

#extracting the response variable
y_qda <- small_X_qda["room_type"]

#getting rid of the response variable and neighbourhood_cleansed
small_X_qda <- small_X_qda%>%
  select(-c("room_type",  "neighbourhood_cleansed"))

small_X_qda <- small_X_qda[,1:24]
```


We start by assessing the optimal K by conducting k-means for different number of K.  


```{r kmeanss, echo = FALSE }

# Select K with WSS

fviz_nbclust(scale(small_X_transf),kmeans,method="wss",k.max=10)

```

The results exhibited by the elbow graph are unclear. The within-cluster variation decreases smoothly as K increases (natural result). In order to assess the optimal number of K we are going to carry out a silhouette analysis. This technique provides more information regarding the goodness of fit of the clustering structure. This method delivers information not just about the distances within a given group but with respect to other groups. Therefore, well defined groups will yield better results than groups with week boundaries. 

```{r Silhouette, echo = FALSE}
# Select K with Silhouette
fviz_nbclust(scale(small_X_transf),kmeans,method="silhouette",k.max=10)
```

The results suggest that the optimal number of K is equal to two. Notwithstanding, the average silhouette for K 3 is really close to the one obtained for 2. Once again, the results are unclear. In order to get more insights about the optimal number of K we are going to employ the Gap Statistic. This technique compares the within-cluster variation with the expected value under assumption of K=1. 

```{r gap statistic, echo = FALSE}
#Evaluating first local max in gap statistic
gap_stat <- clusGap(scale(small_X_transf),FUN=kmeans,K.max=10,B=10, verbose=FALSE)
fviz_gap_stat(gap_stat,linecolor="steelblue",maxSE=list(method="firstmax",SE.factor = 1))
```

The results provided by the Gap-statistic suggest k=8. However, the results are unclear. There is not a well-defined local maximum. 

```{r cluster solution, include = FALSE}
#running k-means with the optimal amount of clusters identified with silhoutte and gap statistc
kmeans_X <- kmeans(scale(small_X_transf),centers=3,iter.max=1000,nstart=100)

# The cluster solution
kmeans_X$cluster
```

Due to the fact that the previous results were unclear, we are going to conduct the K-means for K=3. Since the silhouette for k=3 was really close to the optimal and the rest of techniques assessed suggest a larger number of k.  

```{r, echo = FALSE}
colors_kmeans_X <- c(color_1,color_2,color_3)[kmeans_X$cluster]
plot(X_pcs$x[,1:2],pch=19,col=colors_kmeans_X,main="First two PCs",xlab="First PC",ylab="Second PC")
```

The plot above shows the distribution of each cluster. It can be seen three well differentiated groups. Two streams of data composed by a different groups each and on top of both streams a third cluster. Unlike the original clustering structure, where let’s recall, we have two streams of data composed mainly by two groups (Private rooms and Entire Home) with a well-defined separation with each stream. In our case, each stream is assigned to a particular group. 

```{r Assessing clusters, echo = FALSE}
#Assessing clusters
sil_kmeans_X <- silhouette(kmeans_X$cluster,dist(small_X[,1:24],"euclidean"))
summary(sil_kmeans_X)
```

The average silhouette obtained for the previous case is really close to 0. This results suggest that the partitions obtained are week.  


Due to the unclear results obtained so far we are going to carry out a clustering structure analysis with the use of PAM and CLARA. PAM and CLARA unlike K-means use the most central point of a group instead of the sample mean. Despite being computational more costly than K-means it allows us to take into consideration categorical variables as well with the use of Gower distances. 


#### PAM
```{r kmedoids, echo = FALSE}
# Run PAM with Manhattan distance 

pam_data <- pam(scale(small_X[,1:24]),k=3,metric="manhattan",stand=FALSE)
colors_pam_X <- c(color_1,color_2,color_3)[pam_data$cluster]
plot(X_pcs$x[,1:2],pch=19,col=colors_pam_X,main="First two PCs",xlab="First PC",ylab="Second PC")
```


The results obtained with PAM differ from the ones obtained with K-means. As it can been seen depicted above, one of the streams of data is assigned to one group, whereas the other stream of data is assigned to the other two groups. The distribution of the clusters within this second stream of data is closer to the original distribution that the one indicated by the K-means algorithm. 


```{r kmedoids2, echo = FALSE}

# Have a look at the silhouette

sil_pam_X <- silhouette(pam_data$cluster,dist(small_X[,1:24],method="manhattan"))
summary(sil_pam_X)
```

As it can been seen in the figure above, the average silhouette obtained for PAM with k=3 is really close to 0 as in previous method. 

As it has been previously stated, PAM also works with mixed data. For this reason, we are going to assess the existence of different clusters with the use of the quantitative variables employed so far in addition to some categorical variables such as: property type, Centro, host_is_superhost, and cancelation_policy.


```{r kmedoids3, include = FALSE}
# Compute the Gower distance for the observations in the data 

small_X_complete$host_is_superhost <- as.factor(small_X_complete$host_is_superhost)
small_X_complete$property_type <- as.factor(small_X_complete$property_type)
small_X_complete$cancellation_policy <- as.factor(small_X_complete$cancellation_policy)
small_X_complete$Centro <- as.factor(small_X_complete$Centro)
small_x_mixed <- cbind(scale(small_X_complete[1:24]), small_X_complete[25:28])

X_Gower <- daisy(small_x_mixed,metric="gower")

summary(X_Gower)

X_Gower_mat <- as.matrix(X_Gower)
X_K <- matrix(NA,nrow=1,ncol=10)
for (i in 1:10){
  pam_X_Gower_mat <- pam(X_Gower_mat,k=i+1,diss=TRUE)
  X_K[i] <- pam_X_Gower_mat$silinfo$avg.width
}
```




```{r kmedoids4, echo = FALSE}
plot(1:10,X_K,pch=19,col="deepskyblue2",xlab="Number of clusters",ylab="Average silhouette")
```

By assessing the average silhouette yielded by the different number of clusters with PAM using mixed data, we observe that the optimal amount of K suggested is equal to 2. 


```{r kmedoids5, include = FALSE}
# Run the algorithm for K=2 and get some information from the results

pam_X_Gower_mat <- pam(X_Gower_mat,k=2,diss=TRUE)

# Medoids

small_X_complete[pam_X_Gower_mat$medoids,]

# Have a look at the silhouette

sil_pam_X_Gower_mat <- silhouette(pam_X_Gower_mat$cluster,X_Gower_mat)
plot(sil_pam_X_Gower_mat,col=color_1)
```



```{r kmedoids6, echo = FALSE}

summary(sil_pam_X_Gower_mat)
```

When conducting PAM with K=2, the average silhouette yielded by the clustering structure defined is 0.32.
 
### CLARA
\
Although Clara is similar to PAM but employed for large data sets, we are going to carry out the analysis with it as well so we can compare results. 


```{r clara1, echo = FALSE}

# Run CLARA with Manhattan distance and K=2

clara_X <- clara(scale(small_X_transf),k=2,metric="manhattan",stand=FALSE)

# Make a plot of the first two PCs with the solution

colors_clara_X <- c(color_1,color_2,color_3)[clara_X$cluster]
plot(X_pcs$x[,1:2],pch=19,col=colors_clara_X,main="First two PCs",xlab="First PC",ylab="Second PC")
```

When conducting Clara with k=2, we observe that each stream of data is assigned to a different group.

```{r clara2, echo = FALSE}

# Have a look at the silhouette

sil_clara_X <- silhouette(clara_X$cluster,dist(small_X_transf,method="manhattan"))

summary(sil_clara_X)
# The solution is exactly the same that the one given by PAM but the computational cost is less

```

Furthermore, the average silhouette exhibited by this clustering structure is around 0.15.  

### Hierarchical Clustering 

Hierarchical clustering is a different approach to K-means clustering for identifying and discovering different groups within a dataset. However, in contrast to k-means, hierarchical clustering will create a hierarchy of clusters and therefore does not require us to choose the number of clusters (k) in advance. Besides this, hierarchical clustering has another added advantage over k-means clustering: its results can be easily visualized using a tree-based graph called a _dendrogram_.

Hierarchical clustering can be divided into two main types:

 1. **Agglomerative clustering:** This method works in a bottom-up manner. That is, each observation is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are a member of just one single big cluster. The result is a tree which can be displayed using a _dendrogram_.
 
 2. **Divisive hierarchical clustering:** DIANA (DIvise ANAlysis) works in a top-down way and can therefore be seen as the reverse of **agglomerative clustering**. It begins with the root, in which all observations are included in a single cluster. At each step of the algorithm, the current cluster is split into two clusters that are considered most heterogeneous. The process is iterated until every observations has its own cluster. The number of clusters will then equal the number of instances.
 
Both of these two methods of hierarchical clustering will be performed on the dataset below.

#### Agglomerative Method \newline

Similar to k-means, the similarity of observations are measured using distance measures (_Euclidean_ or _Manhattan_ distance). The Euclidean distance is most commonly used, but that is not a factor with a strong influence in this method. On the other hand, an important question in hierarchical clustering is: How can the dissimilarity between two clusters of observations be measured? A number of different cluster *linkage methods* have been developed to answer this question. The most common methods are:

 1. **Complete linkage clustering:**  All pairwise distances between the elements in cluster 1 and the elements in cluster 2 are computed, and the **largest** value of these distances is taken as the distance between the two clusters. This method tends to create more compact clusters.
 
 2. **Single linkage clustering:** All pairwise distances between the elements in cluster 1 and the elements in cluster 2 are computed, and the **smallest** value of these distances is taken as the linkage criterion. This methods tends to create long, “loose” clusters.

 3. **Average linkage clustering:** All pairwise distances between the elements in cluster 1 and the elements in cluster 2 are computed, and the **average** value of these distances is taken as the linkage criterion. This method can create both compact or less compact clusters. There is not a general rule of thumb for this.

 4. **Ward’s minimum variance method:** This method minimizes the total within-cluster variance. At each step the pair of clusters with the smallest between-cluster distance is merged. This method tends to produce more compact and equal clusters.
 
```{r Hierarchical-Agglomerative-Single}
K = 3 # based on silhouette

# Compute distances
man_dist_X <- daisy(scale(X),metric="manhattan")

# Single linkage
single_X <- hclust(man_dist_X,method="single")

# Plot dendrogram
plot(single_X,main="Single linkage",cex=0.8)
rect.hclust(single_X,k=4,border=color_1)

cl_single_X <- cutree(single_X,4)
table(cl_single_X)

# Plot of the first two PCs with the five clusters
colors_single_X <- c(color_1,color_2,color_3,color_4,color_5)[cl_single_X]
plot(X_pcs$x[,1:2],pch=19,col=colors_single_X,main="First two PCs",xlab="First PC",ylab="Second PC")

# Silhouette
sil_single_X <- silhouette(cl_single_X,man_dist_X)
plot(sil_single_X,col=color_1)
```

```{r Hierarchical-Agglomerative-Complete}
# Complete linkage
complete_X <- hclust(man_dist_X,method="complete")

# Plot dendrogram of the solution for k=5 as in K-means
plot(complete_X,main="Complete linkage",cex=0.8)
rect.hclust(complete_X,k=5,border=color_1)

cl_complete_X <- cutree(complete_X,5)
table(cl_complete_X)

# Plot of the first two PCs with the five clusters
colors_complete_X <- c(color_1,color_2,color_3,color_4,color_5)[cl_complete_X]
plot(X_pcs$x[,1:2],pch=19,col=colors_complete_X,main="First two PCs",xlab="First PC",ylab="Second PC")

# Silhouette
sil_complete_X <- silhouette(cl_complete_X,man_dist_X)
plot(sil_complete_X,col=color_1)
```

```{r Hierarchical-Agglomerative-Average}
# Average linkage
average_X <- hclust(man_dist_X,method="average")

# Plot dendogram of the solution for k=5 as in K-means
plot(average_X,main="Average linkage",cex=0.8)
rect.hclust(average_X,k=5,border=color_1)

cl_average_X <- cutree(average_X,5)
table(cl_average_X)

# Plot of the first two PCs with the five clusters
colors_average_X <- c(color_1,color_2,color_3,color_4,color_5)[cl_average_X]
plot(X_pcs$x[,1:2],pch=19,col=colors_average_X,main="First two PCs",xlab="First PC",ylab="Second PC")

# Silhouette
sil_average_X <- silhouette(cl_average_X,man_dist_X)
plot(sil_average_X,col=color_1)
```

```{r Hierarchical-Agglomerative-Ward}
# Ward linkage
ward_X <- hclust(man_dist_X,method="ward")

# Plot dendrogram of the solution for k=5 as in K-means
plot(ward_X,main="Ward linkage",cex=0.8)
rect.hclust(ward_X,k=5,border=color_1)

cl_ward_X <- cutree(ward_X,5)
table(cl_ward_X)

# Plot of the first two PCs with the five clusters
colors_ward_X <- c(color_1,color_2,color_3,color_4,color_5)[cl_ward_X]
plot(X_pcs$x[,1:2],pch=19,col=colors_ward_X,main="First two PCs",xlab="First PC",ylab="Second PC")

# Silhouette
sil_ward_X <- silhouette(cl_ward_X,man_dist_X)
plot(sil_ward_X,col=color_1)

# This solution is probably the best one among the agglomerative hierarchical clustering methods
```

Now rises the question: which of the agglomerative hierarchical clustering methods works the best (gives the best results)? Based on the plots shown above, the conclusion could be made that the last method (_Ward Method_) yields the best results. To further ground this conclusion the *Agglomerative Coefficient* (AC) could be used.

```{r Hierarchical-Agglomerative-BestMethod}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(scale(X), method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

The AC shows the strength of the generated clustering structure. The closer the value is to 1, the more balanced the clustering structure is. The _complete_ and _Ward’s_ linkage methods generally yield higher AC values. Values closer to 0 imply less well-formed clusters, as can be seen above in the dendrogram of the _single linkage_. Important is to note that the AC tends to become larger as $n$ increases, so it should only be used to compare the same dataset (with an equal $n$) and should therefore not be used across data sets of very different sizes.

Here we see that Ward’s method has the highest AC which means that it has the strongest clustering structure of the four methods assessed.

#### Divisive Method (DIANA) \newline

\noindent For DIANA, clusters are divided based on the maximum average dissimilarity which is very similar to the mean or average linkage clustering method outlined above.

```{r Hierarchical-Divisive-DIANA}
# Divisive hierarchical clustering
diana_X <- diana(scale(X),metric="manhattan")

# Plot dendrogram of the solution
plot(diana_X,main="DIANA")

# Hit two times Return to see the dendogram
# The heights here are the diameters of the clusters before splitting
# Take k=5

rect.hclust(diana_X,k=5,border=color_1)

cl_diana_X <- cutree(diana_X,5)
table(cl_diana_X)

# Plot of the first two PCs with the five clusters
colors_diana_X <- c(color_1,color_2,color_3,color_4,color_5)[cl_diana_X]
plot(X_pcs$x[,1:2],pch=19,col=colors_diana_X,main="First two PCs",xlab="First PC",ylab="Second PC")

# Silhouette
sil_diana_X <- silhouette(cl_diana_X,man_dist_X)
plot(sil_diana_X,col=color_1)

# Divisive Coeffectient
diana_X$dc
```


The best method of the Hierarchical Clustering analysis seems to be the last one: the _DIANA_ method. As can be seen above is that the divisive coeffectient of `r diana_X$dc` is lower than the aglomerative coefficient of the _Ward Method_. This can be explained, however, by the fact that the _Ward Method_ creates more equal clusters, which drives up the score. Reality is, though, that the data is imbalanced and therefore the clusters do not need to be equal.

### Model Based Clustering 

This method, in contrast to traditional methods such as the partitional or hierarchical methods that compute clusters based on the data, tries to allocate a mixture of distributions to the data by allocating a measure of probability and uncertainty to the cluster assignments.

Model-based clustering tries to provide a *soft-assignment*, which means that observations have a probability of belonging to a specific cluster. This method has also the benefit of automatically identifying the optimal number of clusters.

```{r model-based a, echo=FALSE}
n <- nrow(X)
p <- ncol(X)
Z <- X_pcs$x[,1:2]
```

We start by computing the BIC for the different types of covariance matrices that describe the distributions. The function mclust is able to work with up to 14 different covariance matrix configurations, and makes the computations up to the number of selected possible groups, and then selects the best value along with the best covariance.

At the end, this method will select the top 3 models that have a better fit to the data.

```{r model-based b}
# compute the BIC for up to 5 groups
BIC_X <- mclustBIC(Z,G=1:4)
BIC_X
```

We can see that the chosen models have in all cases 4 clusters. And the types are: VVV (ellipsoidal, varying volume, shape, and orientation), VVI (diagonal, varying volume and shape) and VVE (ellipsoidal, equal orientation). We can see that this model selects always the models that have a higher number of clusters.


Below we plot the graph of the results with the different covariance shapes and the resulting BIC scores. Keep in mind that these results are displayed as the negative of the BIC, thus we select the maximum possible value. We see that the graph is strognly increasing, until it reaches around 5 groups, where the BIC value starts to decrease again.

```{r model-based c, echo=FALSE}
plot(BIC_X)
```

Now we will run the mclust function with the optimal solution obtained in the part above. 

```{r model-based d}
Mclust_X <- Mclust(Z,x=BIC_X)
summary(Mclust_X)
```

And finally, we can plot the classification according to this model-based clustering approach. And we plot also the probabilities of the data belonging to each of the different clusters.

```{r model-based e, echo=FALSE}
plot(Mclust_X,what="classification")
colors_Mclust_X <- c(color_1,color_2,color_3,color_4, color_5)[Mclust_X$classification]
par(mfrow=c(2,2))
plot(1:n,Mclust_X$z[,1],pch=19,col=colors_Mclust_X,main="Cluster 1",xlab="Gene",ylab="Probability of cluster 1")
plot(1:n,Mclust_X$z[,2],pch=19,col=colors_Mclust_X,main="Cluster 2",xlab="Gene",ylab="Probability of cluster 2")
plot(1:n,Mclust_X$z[,3],pch=19,col=colors_Mclust_X,main="Cluster 3",xlab="Gene",ylab="Probability of cluster 3")
plot(1:n,Mclust_X$z[,4],pch=19,col=colors_Mclust_X,main="Cluster 4",xlab="Gene",ylab="Probability of cluster 4")
```

From the probability graphs from each of the clusters, we see that there are some uncertainties in the cluster #1 and #4, since we see that there are more point that stay around the midline and less that go to the extremes (0,1).

To finish, we plot the uncertainty of the points that were assigned in a similar manner as the classification plot shown above. The bigger the points are, the less certain its allocation to its current cluster is.

```{r model-based f}
par(mfrow=c(1,1))
plot(Mclust_X,what="uncertainty")
```

To conclude, we can see that the model-based approach is a very interesting one, since it does not base the clusters on the data itself, but rather builds dome possible mixture of models based on the structure of the covariance matrices of each of the distributions. The name of the model chosen is given by the characteristics and shape of the distribution, some might be ellipsiodal, other are spherical and some can also be diagonal. The package used "mclust" has a total of 14 possible multivariate mixtures. 

We have seen that the multivariate mixture that better fits our case is the VVV, 5. This consists of a multivariate mixture with 5 ellipsoidal shapes with varying volume, shape and orientation, thus it does not really help much on the clarification of the distribution, but it gives us a general understanding that the data has indeed some different clusters that must be considered. 

## 2. Perform supervised classification. Obtain conclusions from the analysis.

This part of the report will show three different methods for performing supervised learning. For this type of learning, we will need a response variable, this will be 'room_type', just as in the previous case. 

The three different methods to be explained are the following: 

* K-nearest-neighbours KNN

* Methods based on the Bayes Theorem
  - Linear Discriminant Analysis (LDA)
  - Quadratic Discriminant Analysis (QDA)
  - Naive Bayes
  
* Logistic Regression

#### KNN

K-nearest-neighbours is a relatively simple algorithm in which each new observation is predicted based on the K nearest neighbours of this new observation. KNN is a memory based algorithm, since it has to store all the training data for making future comparissons, and cannot be described in any closed-form. The Knn algortihm has shown to be very useful, but it can also be somehow computationally inefficient. 

We will show below the process for classifying instances according to the 'room_type' attribute. This attribute, as discussed above, has 4 possible categories. 

```{r KNN a, echo=FALSE}
X = small_X[,1:24]
Y = small_X["room_type"]
n <- nrow(Y)
p <- ncol(X)
```

We must first split the data into training a testing datasets, we have chosen to do a 70-30 strategy, which is commonly used. Below we can see the division of the groups.

```{r KNN b, echo=FALSE}
set.seed(8)
n_train <- floor(0.7*n)
n_test <- n - n_train

sprintf('The training set has %f observations', n_train)
sprintf('The testing set has %f observations', n_test)

i_train <- sort(sample(1:n,n_train))
i_test <- setdiff(1:n,i_train)


X_train <- X[i_train,]
Y_train <- Y[i_train,]

X_test <- X[i_test,]
Y_test <- Y[i_test,]

lab <- c('Entire home/apt', "Private room", "Hotel room", "Shared room")

print('Proportions in the Training set')
for (i in lab){
  print(i)
  print(sum(Y_train==as.character(i))/n_train)
}
print("-------")
print('Proportions in the Testing set')
for (i in lab){
  print(i)
  print(sum(Y_test==as.character(i))/n_test)
}
```

We can see that the proportions of the groups in the testing and training datasets are very similar. We must scale the data before we perform any computations. In this case, we will use the library 'class', which was imported in the beginning.

```{r KNN c}
stan_X_train <- scale(X_train)
stan_X_test <- scale(X_test)
```

Now we run the KNN algorithm by changing the number of neighbours we consider from 3 to 40 neighbours. This code will generate a graph of the number of neighbours considered and the LER (Learning Error Rate). We will then choose the number of neighbours that minimizes this error. 

```{r KNN d}
LER <- matrix(NA,nrow=40,ncol=1)
for (i in 3 : 40){
  knn_output <- knn.cv(stan_X_train,Y_train,k=i)
  LER[i] <- 1 - mean(knn_output==Y_train)
}
plot(1:40,LER,pch=20,col=color_1,type="b",xlab="K",ylab="LER",main="LER for logs of AirBnB dataset")

K <- which.min(LER)
K
knn_Y_test <- knn(stan_X_train,stan_X_test,Y_train,k=K,prob=TRUE)
```

Below we see the confusion matrix, and we can see that the model does not predict the cases of Hotel_Room and Shared_Room, since those two cases represent a minority in the dataset, thus for the model it is very complicated to predict them correctly.

```{r KNN e, echo=FALSE}
# Confusion table
table(Y_test,knn_Y_test)

# Obtain the Test Error Rate (TER)
prob_knn_Y_test <- attributes(knn_Y_test)$prob

knn_TER <- mean(Y_test!=knn_Y_test)
knn_TER

# Make a plot of the probabilities of the winner group
# In blue, good classifications, in red, wrong classifications

colors_errors <- c(color_3,color_1)[1*(Y_test==knn_Y_test)+1]
plot(1:n_test,prob_knn_Y_test,col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Winning probabilities",
     main="Winning probabilities")
```

Overall, we can see that the KNN model does a very good job when predicting the two main classes, since the error rate in those cases are very small. But we can see with the help of the probability graphs that the error rates in the smaller groups is much larger and harder to predict.

#### Methods based on the Bayes Theorem

In this section, we are going to assess the use of a set of classifiers that are based on the bayes rules. By assuming a specific probabilistic framework, the TER is minimized by the Bayes rule method. These classifiers compute the conditional probabilities associated to each class and predicts y with the associated  class that exhibits a higher probability. 

### Linear Discriminant Analysis method

Firstly, we are going to employ the Linear Discriminant Analysis method. This method assumes that densities are gaussian, although we have seen that it can work well in some cases when this assumption does not hold. In our case, although we tried to tackle this issue in the first part of the project, we know that some of our variables are non-gaussian. We transformed them to push them to a more normal distribution but some of them are still not gaussian. 

In order to compute the probabilities we need to compute sample mean vectors and the estimated prior probabilities for each group. For this particular method, the covariance matrix is assumed to be the same across populations.


```{r Bayes Theorem a, include = FALSE}
# Linear Discriminant Analysis (LDA) 

# Estimate the unknown parameters with the training sample

lda_train <- lda(Y_train ~ .,data=X_train)

# Estimated prior probabilities for the four groups

lda_train$prior

# Estimated sample mean vectors

t(lda_train$means)

# The function does not return the estimated covariance matrix

##################################################################################################################
# Classify the observations in the test sample

lda_test <- predict(lda_train, newdata = X_test)

# The vector of classifications made can be found here

lda_Y_test <- lda_test$class
lda_Y_test

# Number of properties classified in each group

table(lda_Y_test)

# Contingency table with good and bad classifications

table(Y_test,lda_Y_test)
```


```{r Bayes Theorem b, include = FALSE}
# Test Error Rate (TER)

lda_TER <- mean(Y_test!=lda_Y_test)
lda_TER

# Posterior probabilities of the classifications made with the test sample

prob_lda_Y_test <- lda_test$posterior

```


Once the conditionals probabilities are computed, for any given observation in the test data set we predict its corresponding class (group) by identifying it with the class that exhibits the largest probability. The TER yielded by the method is around 0.16. 

```{r Bayes Theorem c, echo = FALSE}
# Make a plot of the probabilities of different properties 
# In blue, good classifications, in orange, wrong classifications

#### Probabilities for Entire home/apt
colors_errors <- c(color_3,color_1)[1*(Y_test==lda_Y_test)+1]
plot(1:n_test,prob_lda_Y_test[,1],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Entire home/apt")
abline(h=0.5)
```
In the plot above we observe that the closer the observations are to the extreme probability values 1, 0, the more accurate the predictions are. We also see a high concentration of observations that are properly selected as Entire homes in the rage of 0.8 to 1. 

```{r Bayes Theorem d, echo = FALSE}
#### Probabilities for Hotel rooms
plot(1:n_test,prob_lda_Y_test[,2],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Hotel rooms")
abline(h=0.5)
```

On this case, we observe that the probabilities associated to each observation are really low compared to the previous case. Most of the observations that had an associated probability higher than 0.5 were wrongly selected. Whereas those that exhibited a lower probability were properly selected as not being hotel rooms.


```{r Bayes Theorem e, echo = FALSE}
#### Probabilities for Private rooms
plot(1:n_test,prob_lda_Y_test[,3],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Private rooms")
abline(h=0.5)
```
Similar to the case of Entire homes, we observe that the accuracy is really high for those observations associated to extreme probability values (close to 1 and 0). Whereas most of the errors are given by observations situated in the middle of the plane. 

```{r Bayes Theorem f, echo = FALSE}
#### Probabilities for Shared rooms
plot(1:n_test,prob_lda_Y_test[,4],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Shared rooms")
abline(h=0.5)

```
Finally, for the case of Shared rooms, we observe that the observations associated to high probability values were properly chosen. Whereas the majority of the observations with low probability values were properly selected. Notwithstanding, we find numerous errors in that lowest part of the plane. 

### Quadratic Discriminant Analysis (QDA)

This model is similar to the previous method. However, in this case, we assume that the covariance matrices are different across groups. 

```{r Bayes Theorem2, include = FALSE}
# Quadratic Discriminant Analysis (QDA)

# Obtaining a training data set and test data set with the majority classes

n_qda = nrow(small_X_qda)

# Number of observations in each set

n_train_qda <- floor(0.7*n_qda)
n_test_qda <- n_qda - n_train_qda

# Obtain the indices of the observations in both data sets at random
i_train_qda <- sort(sample(1:n_qda,n_train_qda))
length(i_train_qda)
i_test_qda <- setdiff(1:n_qda,i_train_qda)

small_X_qda_train <- small_X_qda[i_train_qda,]
Y_train_qda <- y_qda[i_train_qda,]
small_X_qda_test <- small_X_qda[i_test_qda,]
Y_test_qda <- y_qda[i_test_qda,]


# Estimate the unknown parameters with the training sample

qda_train <- qda(Y_train_qda ~ .,data=small_X_qda_train)


qda_train$prior

# Estimated sample mean vectors

t(qda_train$means)

# The function does not return the estimated covariance matrices

# Classify the observations in the test sample

qda_test <- predict(qda_train,newdata=small_X_qda_test)

# The vector of classifications made can be found here

qda_Y_test <- qda_test$class
qda_Y_test

# Number of properties classified in each group

table(qda_Y_test)

# Contingency table with good and bad classifications

table(Y_test_qda,qda_Y_test)

# Test Error Rate (TER)

qda_TER <- mean(Y_test_qda!=qda_Y_test)
qda_TER

# Posterior probabilities of the classifications made with the test sample

prob_qda_Y_test <- qda_test$posterior
head(prob_qda_Y_test)
```


```{r Bayes Theorem2 b, echo = FALSE, warning = FALSE}
# Make a plot of the probabilities for each type of property
# In blue, good classifications, in orange, wrong classifications

#### Probabilities of Entire home/apt
colors_errors <- c(color_3,color_1)[1*(Y_test==qda_Y_test)+1]
```

Unlike the previous case, in order to be able to compute the QDA method we had to employ a dataset with just the majority classes (Entire homes and private rooms). Due to the imbalance structure of our dataset, the computation was not feasible.

The TER obtained after running this method is 0.16. 


```{r, include = FALSE}
qda_TER
```


```{r Bayes Theorem2 c, echo = FALSE}
plot(1:n_test_qda,prob_qda_Y_test[,1],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities of Entire home/apt")
abline(h=0.5)
```

In the plot above we observe that while in the region of large probabilities there is an accumulation of properly identified classes, in the region of low probabilities we find that the majority of the observations were identified as private rooms despite being Entire homes. 

```{r Bayes Theorem2 d, echo = FALSE}
#### "Probabilities of Private room"
plot(1:n_test_qda,prob_qda_Y_test[,2],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities of Private room")
abline(h=0.5)

```

The results observed above are mirroring the results found for the case of Entire houses. This is to say, we find many observations that were wrongly identified in the large probability region, and many observations properly identified in the low probability region. 

###Naive Bayes (NB) 

This method is a particular case of the Quadratic Discriminant Analysis. Unlike in the previous case, we assume that the predictors are independent and therefore, we do need to compute the variances and we reduce the number of estimates to compute. 

```{r Bayes Theorem2 e, include = FALSE}
# Naive Bayes (NB) 

#converting dataset into a matrix in order to make use of gaussian_naive_bayes()
small_x_train_nb <- as.matrix(X_train)
small_x_test_nb <- as.matrix(X_test)


nb_train <- gaussian_naive_bayes(small_x_train_nb,Y_train)
# Estimated prior probabilities

nb_train$prior

# Estimated sample mean vectors

t(nb_train$params$mu)

# The function does not return the estimated covariance matrices

t(nb_train$params$sd)

##################################################################################################################
# Classify the observations in the test sample

nb_test <- predict(nb_train,newdata=small_x_test_nb,type="prob")

# The vector of classifications made can be found here

nb_Y_test <- c("Entire home/apt", "Hotel room", "Private room", "Shared room")[apply(nb_test,1,which.max)]

nb_Y_test

# Number of properties classified in each group

table(nb_Y_test)
# Contingency table with good and bad classifications

table(Y_test, nb_Y_test)

# Test Error Rate (TER)

nb_TER <- mean(Y_test!=nb_Y_test)
nb_TER

# Posterior probabilities of the classifications made with the test sample

prob_nb_Y_test <- nb_test
head(prob_nb_Y_test)
```

The TER exhibited by the Naïve method is around 0.7, considerably higher than in previous cases.

```{r, include = FALSE}
nb_TER
```


```{r Bayes Theorem2 f, echo = FALSE}
# Make a plot of the probabilities


# Plot of the probabilities for Entire home/apt
colors_errors <- c(color_3,color_1)[1*(Y_test==nb_Y_test)+1]
plot(1:n_test,prob_nb_Y_test[,1],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Entire home/apt")
abline(h=0.5)
```

Unlike in the previous methods considered, we find that for Entire homes, most of the observations that were in the large probability region were properly labelled. Whereas the majority of instances that were located in the low region were wrongly selected as Entire homes. 

```{r Bayes Theorem2 g, echo = FALSE}
# Plot of the probabilities for Hotel rooms
plot(1:n_test,prob_nb_Y_test[,2],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Hotel rooms")
abline(h=0.5)
```

In the graph above we see that most of the observations associated with large probability values were wrongly selected as Hotel Rooms. Similarly, many instances with really low probability values were selected otherwise wrongly. 

```{r Bayes Theorem2 h, echo = FALSE}
# Plot of the probabilities for Private rooms
plot(1:n_test,prob_nb_Y_test[,3],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Private rooms")
abline(h=0.5)
```

For the case of Private rooms, results are unclear. We observe a high dispersion of accurate classifications as well as errors across the plane. 

```{r Bayes Theorem2 j, echo = FALSE}
#Plot of the probabilities for Shared rooms
plot(1:n_test,prob_nb_Y_test[,4],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for for Shared rooms")
abline(h=0.5)

```

Finally, in the case of Shared rooms, we observe that the majority of instances located in the high probability region were misclassified. 

#### Logistic Regression \newline

Linear regression is used to approximate the linear relationship between a continuous response variable and a number of predictor variables. However, when the response variable is categorical linear regression can't be used. However, from a small adaptation to the linear regression method the _logistic regression_ method was born. This method is similar to linear regression in many ways, but are used for binary and categorical response variables instead of continuous ones.

Generally the _logistic regression_ method is used for binary variables, but can also be used for multinomial problems. Since the response variable in this case is not binary (it can take on 4 different values), a multinomial logistic regression will be performed.

```{r Logistic Regression a, results="hide"}
train = cbind(X_train, Y_train)
names(train)[25] = "roomtype"
lr_train <- multinom(roomtype ~ .,data=train)
lr_test <- predict(lr_train,newdata=X_test)
```

```{r Logistic Regression a2}
# Number of rooms classified as each type
summary(lr_test)

# Confusion table
table(Y_test,lr_test)

# Obtain the Test Error Rate (TER)
lr_TER <- mean(Y_test!=lr_test)
lr_TER
```

The probabilities for each of the room types is plot below.

```{r Logistic Regression a3, echo=FALSE}
# Obtain the probabilities of the first level
prob_lr_test <- predict(lr_train,newdata=X_test,type ="probs")

# Make a plot of the probabilities
# In blue, good classifications, in orange, wrong classifications
colors_errors <- c(color_3,color_1)[1*(Y_test==lr_test)+1]

# Plot of the probabilities for Entire home/apt
plot(1:n_test,prob_lr_test[,1],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Entire home/apt")
abline(h=0.5)

# Plot of the probabilities for Hotel rooms
plot(1:n_test,prob_lr_test[,2],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Hotel rooms")
abline(h=0.5)

# Plot of the probabilities for Private rooms
plot(1:n_test,prob_lr_test[,3],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Private rooms")
abline(h=0.5)

#Plot of the probabilities for Shared rooms
plot(1:n_test,prob_lr_test[,4],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for for Shared rooms")
abline(h=0.5)
```

As we can see above, the method has a prediction error of `r lr_TER` which is already not very bad. Let's see if this can be improved by running a backward stepwise algorithm that will select which predictors will be the best at predicting and result in the best predicting score.

```{r Logistic Regression b1, results="hide"}
# Try to improve the test error rate by deleting predictors without discriminatory power
step_lr_train <- step(lr_train,direction="backward",trace=0)
```

```{r Logistic Regression b2}
# Have a look at the variables that have been retained in the model
summary(step_lr_train)$coefnames
```

As can be seen above, the algorithm as only selected `r length(summary(step_lr_train)$coefnames)` out of the total of `r length(X)` predictors. Let's try to make some predictions with this new model.

```{r Logistic Regression c}
# Classify the responses in the test data set with this new model
step_lr_test <- predict(step_lr_train,newdata=X_test)

# Number of rooms classified as each type
summary(step_lr_test)

# Confusion table
table(Y_test,step_lr_test)

# Obtain the Test Error Rate (TER)
step_lr_TER <- mean(Y_test!=step_lr_test)
step_lr_TER
# This model is slightly better than the original one
```

And the probabilities of the room types once again:

```{r Logistic Regression c2, echo=FALSE}
# Obtain the probabilities of the first level
prob_step_lr_test <- predict(step_lr_train,newdata=X_test,type ="probs")

# Make a plot of the probabilities
# In blue, good classifications, in orange, wrong classifications
colors_errors <- c(color_3,color_1)[1*(Y_test==step_lr_test)+1]

# Plot of the probabilities for Entire home/apt
plot(1:n_test,prob_step_lr_test[,1],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Entire home/apt")
abline(h=0.5)

# Plot of the probabilities for Hotel rooms
plot(1:n_test,prob_step_lr_test[,2],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Hotel rooms")
abline(h=0.5)

# Plot of the probabilities for Private rooms
plot(1:n_test,prob_step_lr_test[,3],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for Private rooms")
abline(h=0.5)

#Plot of the probabilities for Shared rooms
plot(1:n_test,prob_step_lr_test[,4],col=colors_errors,pch=20,type="p",xlab="Test sample",ylab="Probabilities",
     main="Probabilities for for Shared rooms")
abline(h=0.5)
```

This newer model has an even lower prediction error of `r step_lr_TER`.

## Conclusion

We have shown throught the report different methods and algorithms for performing unsupervised and supervised classification. We have seen that there exist many different techniques to approach each of the two mehotds. \n

Unsupervised classification does not know in advance which are the classifier groups, nor how many of them there are. Thus it is really interesting to see how the algortihms choose the groups in a completely different way than what we could expect. But this might be due the fact that there are some differences that are classified as more meaningful than others. For example, a private room and a hotel room might have different names, but maybe their overall characteristics are very similar, thus a unsupervised classification method could classify them together. Overall this method is normally used when we do not know in advance if there exist any type of groups, and how are they split. \n

Supervised classification is a machine learning technique, in which the algorithm trains with a set of predictors and a response variable. In our case the response variable is a categorical one. So there exist many different methods to best predict the category given a set of predictors only. We have shown in the report different methods like K-nearest-neighbours, models based on Bayes-Theorems, and finally the logistic regression. We have shown that the TER (Testing Error Rate) of all models is very similar. But we can never say that one method is always better, thus we must try all posible methods and compare their TER's before giving a final answer. \n

Overall, in this assigment we have learnt and applied new techniques and methods to perform both supervised and unsupervised learning. We can see that there are definetely many ways of achieving the goal, and there is never a single method that outperforms all other, thus always trying many of them and comparing is considered one of the best approaches. On a last note, the dataset that we have selected for the project is a clear example of an imbalanced dataset, since out of the 4 existing classes, two of them represented over 85% of the whole sample, and the pther two accounted only for the remaining 15%. Therefore, the proportion of mistakes in the smaller groups tends to be larger than the ones in the larger groups. We decided to leave the proportion of the categories untouched, in order to represent the reality of the dataset.

### Bibliography

This report has been done with help of the lecture notes in:

* Topic 3, Unsupervised Classification, Pedro Galeano, Department of Statistics UC3M-Santander Big Data Institute Universidad Carlos III de Madrid.

* Topic 4, Supervised Classification, Pedro Galeano, Department of Statistics UC3M-Santander Big Data Institute Universidad Carlos III de Madrid.

And the some extra comments and information have been taken from: 

* Hands-On Machine Learning with R, Bradley Boehmke & Brandon Greenwell, 2020-02-01




  
