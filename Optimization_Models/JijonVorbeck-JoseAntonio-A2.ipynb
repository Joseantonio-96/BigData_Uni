{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:helvetica;grey:blue;font-size:18px;line-height: 1.4em;text-align:center;\">\n",
    "    </b></span><br>\n",
    "    <span style=\"color:grey;font-size:32px\"><b><u>UC3M Master in Big Data Analytics</u> </b></span><br>\n",
    "    </b></span><br>\n",
    "    <span style=\"color:grey\"><b>Optimization for large-scale Data</b></span><br>\n",
    "    </b></span><br>\n",
    "    <span style=\"color:grey\"><b>Assignment 2</b></span><br>\n",
    "    </b></span><br>\n",
    "    <span style=\"color:grey;font-size:15px\"><b>Jose Antonio Jijon Vorbeck - 100438045</b></span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import randint\n",
    "from numpy.linalg import inv\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from numpy import linalg as LA\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start by generating the random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 coeffs of betas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [-5],\n",
       "       [-5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K = 100 \n",
    "n_pred = 100\n",
    "\n",
    "# 1000 observations\n",
    "n_obs = 1000\n",
    "\n",
    "# generate random beta coefficients between (-5,5) 100 in total\n",
    "betas = randint(-5,5,size=([n_pred+1,1]))\n",
    "\n",
    "# generate the matrix X using random numbers\n",
    "\n",
    "X0_s = np.ones([n_obs,1]) # coefficients of beta_0\n",
    "X1_s = np.random.uniform(-10,10,[n_obs, n_pred]) # rest of the X matrix size(n_obs, n_pred)\n",
    "X = np.concatenate([X0_s, X1_s], axis = 1) # put together X_0s and X_1s\n",
    "\n",
    "\n",
    "# The errors must be normally distriuted around 0\n",
    "error = np.random.normal(0,1,(n_obs,1))\n",
    "\n",
    "# now we can compute the Y_s\n",
    "Y = np.dot(X,betas) + error\n",
    "print('First 5 coeffs of betas')\n",
    "betas[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the assignment is to adjust a multiple linear regression model to explain variable $Y$ in terms of the other variables $X$, having\n",
    "\n",
    "$Y = \\beta'X + \\epsilon$ where $\\beta = (\\beta_0, \\beta_1, ... \\beta_K )$\n",
    "\n",
    "By using the *Ridge regression*\n",
    "\n",
    "\\begin{align*}\n",
    "  \\text{minimize}_\\beta \\quad  \\| y - X\\beta \\|_2^2 + \\rho \\| \\beta \\|_2^2\n",
    "\\end{align*}\n",
    "\n",
    "We can consider $\\rho$ to be a fixed parameter, e.g., $\\rho= 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Questions </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.) Estimate the value of the regression coefficients by implementing the analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical solution of the normal linear regression is given by: \n",
    "\n",
    "$\\beta_{ls}=(X^T X)^{-1}X^T y$\n",
    "\n",
    "In the Ridge regression, the equation in matrix form is the following: \n",
    "\n",
    "$(Y - X\\beta)^T(Y - X\\beta) + \\rho\\beta^T\\beta$\n",
    "\n",
    "So, by taking its derivative and deriving with respect to $\\beta$:\n",
    "\n",
    "$X^TY = (X^TX + \\rho I)\\beta$\n",
    "\n",
    "And we get to the analytical solution of the Ridge equation:\n",
    "\n",
    "$\\beta = (X^TX + \\rho I)^{-1}X^TY$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.01073\n",
      "Absolute mean error from true betas: 0.0058\n",
      "Values of first 5 coeff. analytically calculated:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.06249336],\n",
       "       [ 1.01259877],\n",
       "       [ 2.00723955],\n",
       "       [-5.00696314],\n",
       "       [-5.00186935]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We set rho to be fixed at 1\n",
    "rho = 1\n",
    "start = time.time()\n",
    "beta_analytical = np.dot(np.dot(inv(np.dot(X.T,X) + rho*np.identity(n_pred+1)),X.T),Y)\n",
    "time_analytic = round(time.time() - start,5)\n",
    "print(f'Time elapsed: {time_analytic}')\n",
    "print(f'Absolute mean error from true betas: {round(np.mean(np.abs(beta_analytical-betas)),5)}')\n",
    "print('Values of first 5 coeff. analytically calculated:')\n",
    "beta_analytical[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to compute future errors:\n",
    "def error_value(beta_reg):\n",
    "    return LA.norm(beta_analytical.T-beta_reg,ord=2) / LA.norm(beta_analytical.T,ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.) Estimate the value of the regression coefficients by using the function minimize from the Python module Scipy.optimize. Try at least four available solvers and compare their performance in terms of number of iterations, number of function, gradient and hessian evaluations as well as total computational time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is simply given by minimizing the following function: \n",
    "\n",
    "$\\| y - X\\beta \\|_2^2 + \\rho \\| \\beta \\|_2^2$\n",
    "\n",
    "For the computation of the Gradient, we need to apply the first derivative with respect to $\\beta$\n",
    "\n",
    "We will write, for simplicity, in scalar notation: <br>\n",
    "$(y - x\\beta)^2 + \\rho\\beta^2$\n",
    "\n",
    "Deriving this w.r.t. $\\beta$ we get the following: \n",
    "\n",
    "$-2(y - x\\beta)x + 2\\rho\\beta$ (1), which simpplifies to: \n",
    "\n",
    "$-2yx + 2x^2\\beta + 2\\rho\\beta$, deriving this again w.r.t. $\\beta$ \n",
    "\n",
    "$2x^2 + 2\\rho$\n",
    "\n",
    "So, in matrix notation we are left with the following: \n",
    "\n",
    "**Gradient:**   <br> $-2(Y - X\\beta)X + 2\\rho\\beta$  \n",
    "**Hessian:**  <br>  $2X^TX + 2\\rho I$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the Functions:\n",
    "\n",
    "Here I will define the functions for use in the rest of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the objective function: \n",
    "def least_sq_ridge(beta_ls, X, Y, rho):\n",
    "    beta_ls = np.matrix(beta_ls)\n",
    "    z = Y - np.dot(X,beta_ls.T)\n",
    "    f = np.dot(z.T,z) + rho * LA.norm(beta_ls)\n",
    "    return f\n",
    "\n",
    "# function for the jacobian (gradient) matrix\n",
    "def gradient(beta_ls, X, Y, rho):\n",
    "    beta_ls = np.matrix(beta_ls)\n",
    "    z = Y - np.dot(X,beta_ls.T)\n",
    "    gr = -2*np.dot(z.T,X) + 2*rho*beta_ls \n",
    "    aa = np.squeeze(np.asarray(gr))\n",
    "    return aa\n",
    "\n",
    "# function for the hessian matrix\n",
    "def hessian(beta_ls, X, Y, rho):\n",
    "    beta_ls = np.matrix(beta_ls)\n",
    "    hes = 2*np.dot(X.T,X) + 2*rho*np.identity(n_pred+1)\n",
    "    return np.squeeze(np.asarray(hes))\n",
    "\n",
    "# coordinate gradient function\n",
    "def gradient_coord(beta_ls, index, X, Y, rho):\n",
    "    beta_ls = np.matrix(beta_ls)\n",
    "    pp = -2*np.dot((Y-np.dot(X,beta_ls.T)).T,X[:,index]) + 2*rho*beta_ls[:,index]\n",
    "    aa = np.zeros(b)\n",
    "    aa[index]=pp\n",
    "    return aa\n",
    "\n",
    "# initiate all coeff. to zero\n",
    "beta_0 = np.zeros(n_pred+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply the minimization function with 5 different solvers.<br>\n",
    "The solutions and summary table of each will be presented in a table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of function evaluations has been exceeded.\n"
     ]
    }
   ],
   "source": [
    "# Nelder_Mead\n",
    "start = time.time()\n",
    "reg_Nelder_Mead = minimize(least_sq_ridge, beta_0, args=(X, Y, rho), method='Nelder-Mead', options={'disp': True,'xtol': 1e-10})\n",
    "Nelder_Mead_time = time.time() - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nelder_Mead_sum = {'time': Nelder_Mead_time, 'error': error_value(reg_Nelder_Mead.x), 'iterations': reg_Nelder_Mead.nit,\n",
    "                  'function evals': reg_Nelder_Mead.nfev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 858.716329\n",
      "         Iterations: 8\n",
      "         Function evaluations: 15036\n"
     ]
    }
   ],
   "source": [
    "# Powell\n",
    "start = time.time()\n",
    "reg_Powell = minimize(least_sq_ridge, beta_0, args=(X, Y, rho), method='Powell', options={'disp': True,'xtol': 1e-10})\n",
    "Powell_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Powell_sum = {'time': Powell_time, 'error': error_value(reg_Powell.x), 'iterations': reg_Powell.nit,\n",
    "                  'function evals': reg_Powell.nfev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COBYLA\n",
    "start = time.time()\n",
    "reg_COBYLA = minimize(least_sq_ridge, beta_0, args=(X, Y, rho), method='COBYLA', options={'disp': True})\n",
    "COBYLA_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "COBYLA_sum = {'time': COBYLA_time, 'error': error_value(reg_COBYLA.x),\n",
    "                  'function evals': reg_COBYLA.nfev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TNC\n",
    "start = time.time()\n",
    "reg_TNC = minimize(least_sq_ridge, beta_0, args=(X, Y, rho), method='TNC', options={'disp': True})\n",
    "TNC_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TNC_sum = {'time': TNC_time, 'error': error_value(reg_TNC.x), 'iterations': reg_TNC.nit,\n",
    "                  'function evals': reg_TNC.nfev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 858.746341\n",
      "         Iterations: 11\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 22\n",
      "         Hessian evaluations: 11\n"
     ]
    }
   ],
   "source": [
    "# Newton-CG\n",
    "start = time.time()\n",
    "reg_Newton_CG = minimize(least_sq_ridge, beta_0, args=(X, Y, rho), hess = hessian, jac = gradient, method='Newton-CG', options={'disp': True})\n",
    "Newton_CG_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newton_CG_sum = {'time': Newton_CG_time, 'error': error_value(reg_Newton_CG.x), 'iterations': reg_Newton_CG.nit,\n",
    "                  'function evals': reg_Newton_CG.nfev, 'gradient evals': reg_Newton_CG.njev, 'hessian evals': reg_Newton_CG.nhev }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 858.745562\n",
      "         Iterations: 118\n",
      "         Function evaluations: 220\n",
      "         Gradient evaluations: 220\n"
     ]
    }
   ],
   "source": [
    "# BFGS\n",
    "start = time.time()\n",
    "reg_BFGS = minimize(least_sq_ridge, beta_0, args=(X, Y, rho), jac = gradient, method='BFGS', options={'disp': True})\n",
    "BFGS_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFGS_sum = {'time': BFGS_time, 'error': error_value(reg_BFGS.x), 'iterations': reg_BFGS.nit,\n",
    "                  'function evals': reg_BFGS.nfev, 'gradient evals': reg_BFGS.njev}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nelder_Mead</th>\n",
       "      <th>Powell</th>\n",
       "      <th>Cobyla</th>\n",
       "      <th>TNC</th>\n",
       "      <th>Newton_CG</th>\n",
       "      <th>BFGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>time</td>\n",
       "      <td>3.87</td>\n",
       "      <td>3.225</td>\n",
       "      <td>0.964</td>\n",
       "      <td>5.796</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>error</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iterations</td>\n",
       "      <td>19527.00</td>\n",
       "      <td>8.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.000</td>\n",
       "      <td>11.00</td>\n",
       "      <td>118.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>function evals</td>\n",
       "      <td>20200.00</td>\n",
       "      <td>15036.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>595.000</td>\n",
       "      <td>12.00</td>\n",
       "      <td>220.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gradient evals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.00</td>\n",
       "      <td>220.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hessian evals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Nelder_Mead     Powell    Cobyla      TNC  Newton_CG     BFGS\n",
       "time                   3.87      3.225     0.964    5.796       0.02    0.095\n",
       "error                  0.79      0.000     0.052    0.000       0.00    0.000\n",
       "iterations         19527.00      8.000       NaN   57.000      11.00  118.000\n",
       "function evals     20200.00  15036.000  1000.000  595.000      12.00  220.000\n",
       "gradient evals          NaN        NaN       NaN      NaN      22.00  220.000\n",
       "hessian evals           NaN        NaN       NaN      NaN      11.00      NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of solvers b.)\n",
    "Summary_b = {'Nelder_Mead': Nelder_Mead_sum, 'Powell': Powell_sum, 'Cobyla': COBYLA_sum, 'TNC': TNC_sum,\n",
    "             'Newton_CG': Newton_CG_sum, 'BFGS': BFGS_sum}\n",
    "pd.DataFrame.from_dict(Summary_b).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the two last methods, for which the jacobian or the hessian matrices are used, converge faster and need less itertions. In fact, the Nelder-Mead needs much more iterations, and the TNC solver requires  way more time to deliver a solution, while the Newton one does it in less than 0.05sec. \n",
    "\n",
    "We see that the errors are also very small for the Newton-CG and the BFGS solvers, compared to the analytical solution obtained in part a.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Estimate the value of the regression coefficients by implementing the following:\n",
    "* Gradient Method\n",
    "* Newton's Method\n",
    "* Quasi-Newton Method\n",
    "\n",
    "*Consider a line search technique to improve the algorithm convergence, e.x., Armijo rule. Compare the performance of these algorithms (number of iterations and total computational time).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we calculate the objective funtion value from the analytically calculated $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objetive function from the analytical beta --> [[858.745562]]\n"
     ]
    }
   ],
   "source": [
    "# objetive function from the analytical beta's\n",
    "print(f'Objetive function from the analytical beta --> {least_sq_ridge(beta_analytical.T, X, Y, rho)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Method**\n",
    "\n",
    "In a few words, this method evaluates the function at an initial given point, and with the help of the negative gradient of the function ($-\\nabla f$) the solver will choose the descent direction, and then move the point to obtain the value until we get to a minimum value, where $\\nabla f --> 0 $ .\n",
    "\n",
    "It is the simplest method and with the least computational cost, but in practice it converges in too many iterations.\n",
    "\n",
    "The mathematical representation of the gradient method is the following: \n",
    "\n",
    "$x_{k+1} = x_{k} -\\nabla f(x_k) * \\alpha_k$\n",
    "\n",
    "Where alpha is the step length, that can also be adjusted at every iteration $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size od the X matrix\n",
    "(a,b) = X.shape\n",
    "\n",
    "# parameters of the algorithm\n",
    "n_iter = 4000 # max number of iterations\n",
    "alpha = 1e-7 # fixed value of alpha\n",
    "eps = 1e-3 # smallest difference\n",
    "tol = 10000 # initial tolerance\n",
    "\n",
    "# initialize the values at 0\n",
    "beta_grad = np.zeros(b)\n",
    "obj_G = np.zeros(n_iter)\n",
    "tol_iterG = np.zeros(n_iter)\n",
    "alpha_k = np.zeros(n_iter)\n",
    "\n",
    "k = 0\n",
    "rho = 1\n",
    "start = time.time()\n",
    "while (tol > eps) and (k <= n_iter-2):\n",
    "    k = k + 1\n",
    "    grad = gradient(beta_grad, X, Y, rho)\n",
    "    \n",
    "    ############### adjusting alpha with the armijo rule\n",
    "    sigma = 0.01\n",
    "    beta= 0.1\n",
    "    alpha=1e-5\n",
    "    while (least_sq_ridge(beta_grad - grad * alpha, X,Y,rho) > least_sq_ridge(beta_grad, X,Y,rho)+alpha*sigma*np.dot(grad,-grad.T)):\n",
    "        alpha = alpha*beta\n",
    "    ################\n",
    "    \n",
    "    beta_grad = beta_grad - grad * alpha\n",
    "    tol = LA.norm(grad,ord=2)\n",
    "    obj_G[k] = least_sq_ridge(beta_grad, X, Y, rho)\n",
    "    tol_iterG[k] = tol\n",
    "    alpha_k[k] = alpha\n",
    "\n",
    "gradient_time = time.time()-start\n",
    "gradient_iterations = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 0.25242090225219727,\n",
       " 'iterations': 843,\n",
       " 'error': 1.852309131998861e-08,\n",
       " 'objective function': 858.7455644066168,\n",
       " \"last iteration's tol\": 0.0009997904113138805}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_sum = {'time': gradient_time, 'iterations':gradient_iterations, 'error': error_value(beta_grad),\n",
    "                'objective function':obj_G[k], \"last iteration's tol\": tol_iterG[k]  }\n",
    "gradient_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Newton's Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netwon's method is a modification of the gradient method, with the addition of the second derivative term of the function, or in matrix representation, what we know as the hessian matrix. \n",
    "\n",
    "It is known to converge very fast and with less iterations than the gradient method, but due to the computation of the gradient and hessian matrices, it can be very computationally costly. It converges always whenever $\\nabla^2f(x_k)$ is not singular.\n",
    "\n",
    "The mathematical representation of this method is the following: \n",
    "\n",
    "$x_{k+1} = x_{x} - \\frac{\\nabla f(x_k)}{\\nabla^2 f(x_k)} \\alpha_k$  \n",
    "\n",
    "$\\rightarrow$ whenever $\\nabla^2 f(x_k)$ is non-singular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size od the X matrix\n",
    "(a,b) = X.shape\n",
    "\n",
    "# parameters of the algorithm\n",
    "n_iter = 4000 # max number of iterations\n",
    "alpha = 1e-7 # starting value of alpha\n",
    "eps = 1e-3 # smallest difference\n",
    "tol = 10000 # initial tolerance\n",
    "sigma = 0.1\n",
    "beta= 0.1\n",
    "\n",
    "\n",
    "# initialize the values at 0\n",
    "beta_newt = np.zeros(b)\n",
    "obj_N = np.zeros(n_iter)\n",
    "tol_iterN = np.zeros(n_iter)\n",
    "alpha_k = np.zeros(n_iter)\n",
    "\n",
    "k = 0\n",
    "rho = 1\n",
    "start = time.time()\n",
    "\n",
    "while (tol > eps) and (k <= n_iter-2):\n",
    "    k = k + 1\n",
    "    grad = gradient(beta_newt, X, Y, rho)\n",
    "    hess = hessian(beta_newt, X, Y, rho)\n",
    "    p_k = -np.dot(np.linalg.inv(hess),grad)\n",
    "    alpha=1\n",
    "    ################ adjusting alpha with the armijo rule\n",
    "    while (least_sq_ridge(beta_newt + p_k*alpha,X,Y,rho) > least_sq_ridge(beta_newt,X,Y,rho)+alpha*sigma*np.dot(p_k,grad)):\n",
    "        alpha = alpha*beta\n",
    "    #################\n",
    "    beta_newt = beta_newt + alpha*p_k\n",
    "    tol = LA.norm(grad,ord=2)\n",
    "    obj_N[k] = least_sq_ridge(beta_newt, X, Y, rho)\n",
    "    tol_iterN[k] = tol\n",
    "    alpha_k[k] = alpha\n",
    "\n",
    "newton_time = time.time()-start\n",
    "newton_iterations = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 0.0045549869537353516,\n",
       " 'iterations': 2,\n",
       " 'error': 4.4017573606486097e-16,\n",
       " 'objective function': 858.7455620027389,\n",
       " \"last iteration's tol\": 1.2445301703958731e-09}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netwon_sum = {'time': newton_time, 'iterations':newton_iterations, 'error': error_value(beta_newt),\n",
    "              'objective function': obj_N[k], \"last iteration's tol\": tol_iterN[k] }\n",
    "netwon_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quasi-Newton's method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the computation of the hessian matrix $\\nabla^2 f(x_k)$ can be very expensive, or in some cases can be singular which would prevent us running the algortihm, the Quasi-Newton methods allow us to avoid this computation by performing some approximations to the Hessian Matrix. \n",
    "\n",
    "We start by an initial value $B_0$ to the matrix $H_0 = \\nabla^2 f(x_0)$, and then iterate to find the optimal case with the following formula: \n",
    "\n",
    "$B_{k+1} = B_k +$ update rule\n",
    "\n",
    "$\\rightarrow$ I will choose the **BFGS method**, for which the update rule is the following: \n",
    "\n",
    "$B_{k+1} = B_k + \\frac{(B_k s_k)(B_k s_k)^T}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}$\n",
    "\n",
    "$\\rightarrow$ where: <br>\n",
    "$s_k = x_{k+1} - x_k$ and <br>$y_k = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$\n",
    "\n",
    "$\\rightarrow$ And for we will use: $B_0 = hessian(0)$,  we will compute the hessian only once at the beginning of the algorithm as a starting point. \n",
    "\n",
    "The computation of the hessian for the step 0 helps a lot with the speed of convergence of the algorithm, since it has already a good starting point. And starting with the hessian matrix does not violate the point of the solver, since we are not computing the hessian matrix iteratively, but only once at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size od the X matrix\n",
    "(a,b) = X.shape\n",
    "\n",
    "# parameters of the algorithm\n",
    "n_iter = 4000 # max number of iterations\n",
    "alpha = 1e-10 # starting value of alpha\n",
    "eps = 1e-3 # smallest difference\n",
    "tol = 10000 # initial tolerance\n",
    "sigma = 0.001\n",
    "beta= 0.8\n",
    "\n",
    "\n",
    "# initialize the values at 0\n",
    "beta_q_newt = np.zeros(b)\n",
    "obj_QN = np.zeros(n_iter)\n",
    "tol_iterQN = np.zeros(n_iter)\n",
    "alpha_k = np.zeros(n_iter)\n",
    "\n",
    "k = 0\n",
    "rho = 1\n",
    "start = time.time()\n",
    "\n",
    "# initial values for B_k, s_k, y_k\n",
    "\n",
    "grad = gradient(beta_q_newt, X, Y, rho)\n",
    "y_k = gradient(beta_q_newt - grad*alpha, X, Y, rho).T - gradient(beta_q_newt, X, Y, rho).T\n",
    "s_k = -grad*alpha\n",
    "# compute the initial hessian as starting point\n",
    "B_k = hessian(beta_q_newt,X,Y,rho)\n",
    "\n",
    "while (tol > eps) and (k <= n_iter-2):\n",
    "    \n",
    "    # gradient\n",
    "    grad = gradient(beta_q_newt, X, Y, rho)\n",
    "    # direction\n",
    "    p_k = -np.dot(inv(B_k),grad)\n",
    "    \n",
    "    ############### adjusting alpha with the armijo rule\n",
    "    alpha=1\n",
    "    while (least_sq_ridge(beta_q_newt + p_k*alpha,X,Y,rho) > least_sq_ridge(beta_q_newt,X,Y,rho)+alpha*sigma*np.dot(p_k,grad)):\n",
    "        alpha = alpha*beta\n",
    "    ################\n",
    "    \n",
    "    # update s_k \n",
    "    s_k = p_k*alpha\n",
    "    \n",
    "    # update alpha\n",
    "    beta_old = beta_q_newt\n",
    "    beta_q_newt = beta_q_newt + s_k\n",
    "    \n",
    "    # update y_k\n",
    "    y_k = gradient(beta_q_newt, X, Y, rho) - gradient(beta_old, X, Y, rho)\n",
    "\n",
    "    # BFGS\n",
    "    B_k =  B_k - ((np.dot(np.dot(B_k,s_k),np.dot(B_k,s_k).T))/(np.dot(np.dot(s_k.T,B_k),s_k))) + np.dot(y_k,y_k.T)/(np.dot(y_k.T,s_k))\n",
    "    \n",
    "    tol = LA.norm(grad,ord=2)\n",
    "    obj_QN[k] = least_sq_ridge(beta_q_newt, X, Y, rho)\n",
    "    tol_iterQN[k] = tol\n",
    "    alpha_k[k] = alpha\n",
    "    k = k + 1\n",
    "\n",
    "quasi_newton_time = time.time()-start\n",
    "quasi_newton_iterations = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 0.006159782409667969,\n",
       " 'iterations': 2,\n",
       " 'error': 4.4017573606486097e-16,\n",
       " 'objective function': 858.7455620027389,\n",
       " \"last iteration's tol\": 1.2445301703958731e-09}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quasi_netwon_sum = {'time': quasi_newton_time, 'iterations':quasi_newton_iterations, 'error': error_value(beta_q_newt),\n",
    "              'objective function': obj_QN[k-1], \"last iteration's tol\": tol_iterQN[k-1] }\n",
    "quasi_netwon_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see a summary table of the three presented methods along with their resulting characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gradient Method</th>\n",
       "      <th>Newton's Method</th>\n",
       "      <th>Quasi Newton Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>time</td>\n",
       "      <td>0.25242</td>\n",
       "      <td>0.00455</td>\n",
       "      <td>0.00616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iterations</td>\n",
       "      <td>843.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>error</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>objective function</td>\n",
       "      <td>858.74556</td>\n",
       "      <td>858.74556</td>\n",
       "      <td>858.74556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>last iteration's tol</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Gradient Method  Newton's Method  Quasi Newton Method\n",
       "time                          0.25242          0.00455              0.00616\n",
       "iterations                  843.00000          2.00000              2.00000\n",
       "error                         0.00000          0.00000              0.00000\n",
       "objective function          858.74556        858.74556            858.74556\n",
       "last iteration's tol          0.00100          0.00000              0.00000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summary_c = {'Gradient Method': gradient_sum, \"Newton's Method\" : netwon_sum, 'Quasi Newton Method': quasi_netwon_sum}\n",
    "pd.DataFrame.from_dict(Summary_c).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the table above, the newton's method is impresively fast since it converges with only 2 iterations and in  less than 0.007 seconds, we can also see that the last iteration's tolerance is practically zero, and that it gets a very good objective value compared to the solvers in section b.) \n",
    "\n",
    "We also see that the quasi-newton method converges in the same number of iterations and time as the newton's method, and this is due to the starting point using the hessian. If we were not using the true hessian as starting point, but an approximation, the algortihm would take longer to converge to a solution.\n",
    "\n",
    "The gradient method takes longer since it only uses a first order approximation to the solution, causing the convergence to be slower. But nevertheless it delivers a very small error.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.) Estimate the value of the regression coefficients by implementing:\n",
    "* Coordinate gradient method\n",
    "* Mini-batch gradient method\n",
    "    - Study how the mini-batch size may impact the algorithm performance (number of iterations and computational time needed to reach a pre-speciffied tolerance limit)\n",
    "* Mini-batch gradient with momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coordinate gradient method**\n",
    "\n",
    "This method computes the gradient of only one component of the $\\beta$ matrix, therefore it might be faster to compute the gradient, but it might need more itertions to converge since it does not take into account the whole components that make the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a,b)=X.shape\n",
    "\n",
    "#parameters\n",
    "alpha=1e-5\n",
    "n_iter= 70000\n",
    "tol=100;\n",
    "eps=1e-4;\n",
    "\n",
    "# initializing values\n",
    "beta_coord = np.zeros(b)\n",
    "OF_iter = np.zeros(n_iter)\n",
    "tol_iterCG = np.zeros(n_iter)\n",
    "\n",
    "i = 0\n",
    "\n",
    "# time\n",
    "start = time.time()\n",
    "\n",
    "while (i <= n_iter-2) and (tol > eps):\n",
    "    i=i+1\n",
    "    \n",
    "    # randomly selecting the index to choose for the method\n",
    "    k = np.random.randint(b)\n",
    "    gradk = gradient_coord(beta_coord,k,X,Y, rho)\n",
    "    \n",
    "    beta_coord = beta_coord - gradk * alpha\n",
    "    tol = LA.norm(gradk, ord = 2)\n",
    "    \n",
    "    OF_iter[i] = least_sq_ridge(beta_coord, X, Y, rho)\n",
    "    tol_iterCG[i] = tol\n",
    "    \n",
    "    \n",
    "    \n",
    "coord_grad_time = time.time()-start\n",
    "coord_grad_iterations = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 0.6500298976898193,\n",
       " 'iterations': 3732,\n",
       " 'error': 0.03501633519514527,\n",
       " 'objective function': 1825.3480159868623,\n",
       " \"last iteration's tol\": 8.961740670621055e-05}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_grad_sum = {'time': coord_grad_time, 'iterations':coord_grad_iterations, 'error': error_value(beta_coord),\n",
    "              'objective function': OF_iter[i], \"last iteration's tol\": tol_iterCG[i]  }\n",
    "coord_grad_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-batch gradient method**\n",
    "\n",
    "This method is very similar to the coordinate gradient one, but instead of just picking one coordinate each time, we must pick a random \"batch\" of coordinates in every iteration and then calculate the gradient with those columns.\n",
    "\n",
    "We will also study how the size of the mini-batch impacts the performance of the algorithm in terms of time and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(alpha=1e-5, n_iter= 50000, tol=100, eps=1e-4, batch = 50):\n",
    "\n",
    "    # initializing values\n",
    "    (a,b)=X.shape\n",
    "    beta_batch = np.zeros(b)\n",
    "    OF_iter = np.zeros(n_iter)\n",
    "    tol_iterB = np.zeros(n_iter)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    i = 0\n",
    "    while (i <= n_iter-2) and (tol > eps):\n",
    "        i=i+1\n",
    "        \n",
    "        # randomly selecting the index to choose for the method\n",
    "        k = np.random.randint(0,b,batch)\n",
    "        gradk = gradient_coord(beta_batch,k,X,Y, rho)\n",
    "        \n",
    "        beta_batch = beta_batch - gradk * alpha\n",
    "        tol = LA.norm(gradk, ord = 2)\n",
    "        \n",
    "        OF_iter[i] = least_sq_ridge(beta_batch, X, Y, rho)\n",
    "        tol_iterB[i] = tol\n",
    "    \n",
    "    mini_batch_time = time.time()-start\n",
    "    mini_batch_iterations = i\n",
    "    \n",
    "    return mini_batch_time, OF_iter[i], tol_iterB[i], beta_batch, mini_batch_iterations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_time, ObjectiveF, Tolerance, beta_batch, mini_batch_iterations = mini_batch(batch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 0.5699477195739746,\n",
       " 'iterations': 1692,\n",
       " 'error': 2.9755740543298597e-07,\n",
       " 'objective function': 858.7456006006252,\n",
       " \"last iteration's tol\": 8.631710472239313e-05}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch_sum = {'time': mini_batch_time, 'iterations':mini_batch_iterations, 'error': error_value(beta_batch),\n",
    "              'objective function': ObjectiveF, \"last iteration's tol\": Tolerance  }\n",
    "mini_batch_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing the batch size**\n",
    "\n",
    "Now, we will compare the behaviour of the mini-batch coordinated descent as we vary the batch size from 1 to b components (number of coeeficients of $\\beta$). We will take into consideration the time and number of iterations it takes to reach a specified tolerance level.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size parameters\n",
    "maxi = 2*b\n",
    "step = 10\n",
    "time_ellapsed = np.zeros(len(range(5,maxi,step)))\n",
    "iterations = np.zeros(len(range(5,maxi,step)))\n",
    "obj_fun = np.zeros(len(range(5,maxi,step)))\n",
    "error = np.zeros(len(range(5,maxi,step)))\n",
    "\n",
    "# repetition parameters\n",
    "r = 10\n",
    "time_iter = np.zeros(r)\n",
    "iterations_iter = np.zeros(r)\n",
    "obj_fun_iter = np.zeros(r)\n",
    "\n",
    "i = 0\n",
    "for size in range(5,maxi,step):\n",
    "    for j in range(0,r):\n",
    "        time_iter[j], obj_fun_iter[j], tol_iterCG, beta_batch, iterations_iter[j] =  mini_batch(batch = size)\n",
    "    \n",
    "    time_ellapsed[i] = np.mean(time_iter)\n",
    "    iterations[i] = np.mean(iterations_iter)\n",
    "    obj_fun[i] = np.mean(obj_fun_iter)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Objective function')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, figsize=(10,10))\n",
    "ax1.plot(list(range(5,maxi,step)),time_ellapsed) \n",
    "ax1.set_ylabel('Time [s]')\n",
    "ax2.plot(list(range(5,maxi,step)),iterations) \n",
    "ax2.set_ylabel('Number of Iterations')\n",
    "ax3.plot(list(range(5,maxi,step)),obj_fun) \n",
    "ax3.set_ylabel('Objective function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs we can see that the optimal batch size is to choose all of the columns of the matrix at the same time (batch size = number of predictors), which actually translates to the normal gradient descent method. \n",
    "\n",
    "Although it is not very clear since the variation of the results are high, we can observe an increasing trend after the batch size passes batch = 100. And what we can see very clearly is that for values lower than 50 the time and the iterations number is very long. \n",
    "\n",
    "If we were to choose only one attribute (k=1), then we go back to the case of the coordinate descent, which we can observe to be the least efficient one.\n",
    "\n",
    "It is iteresting to note that we can even choose a batch size larger than the number of coefficients, but this does not translate into a faster convergence. Although the objective function reaches a plateau and does not get any smaller. \n",
    "\n",
    "Maybe in the case of having thousends of coefficients ($\\beta_p$) we could expect to see the increase of the bacth size translated into more computation time, and thus lower efficiency. But in our example, we see that the number of parameters is not big enough to consider these kinds of techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-batch gradient with momentum**\n",
    "\n",
    "The mini-batch gradient descent with momentum adds speed to the last iterations, when the algorithm is getting closer to the solution, since it considers the last iteration's gradient value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_batch(alpha=1e-5, n_iter= 50000, tol=100, eps=1e-4, batch = 100, eta = 0.8):\n",
    "\n",
    "    # initializing values\n",
    "    (a,b)=X.shape\n",
    "    beta_mbatch = np.zeros(b)\n",
    "    OF_iter = np.zeros(n_iter)\n",
    "    tol_iterMB = np.zeros(n_iter)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    i = 0\n",
    "    gradk = 0\n",
    "    while (i <= n_iter-2) and (tol > eps):\n",
    "        i=i+1\n",
    "        \n",
    "        # randomly selecting the index to choose for the method\n",
    "        k = np.random.randint(0,b,batch)\n",
    "        gradk = gradient_coord(beta_mbatch,k,X,Y, rho) + eta*gradk\n",
    "        \n",
    "        beta_mbatch = beta_mbatch - gradk * alpha\n",
    "        tol = LA.norm(gradk, ord = 2)\n",
    "        \n",
    "        OF_iter[i] = least_sq_ridge(beta_mbatch, X, Y, rho)\n",
    "        tol_iterMB[i] = tol\n",
    "    \n",
    "    momentum_batch_time = time.time()-start\n",
    "    momentum_batch_iterations = i\n",
    "    \n",
    "    return momentum_batch_time, OF_iter[i], tol_iterMB[i], beta_mbatch, momentum_batch_iterations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_batch_time, ObjectiveF, Tolerance, beta_mbatch, momentum_batch_iterations  = momentum_batch(alpha=1e-5, n_iter= 50000, tol=100, eps=1e-4, batch = 50, eta = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 0.7195923328399658,\n",
       " 'iterations': 1540,\n",
       " 'error': 5.849826107888017e-07,\n",
       " 'objective function': 858.7456379727515,\n",
       " \"last iteration's tol\": 8.914436965045366e-05}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "momentum_batch_sum = {'time': momentum_batch_time, 'iterations': momentum_batch_iterations, 'error': error_value(beta_mbatch),\n",
    "              'objective function': ObjectiveF, \"last iteration's tol\": Tolerance  }\n",
    "momentum_batch_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see a summary table with all the recorded parameters from the three different methods studied in section d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coordinate Descent</th>\n",
       "      <th>Mini-Batch Descent</th>\n",
       "      <th>Momentum-Batch Descent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>time</td>\n",
       "      <td>0.65003</td>\n",
       "      <td>0.56995</td>\n",
       "      <td>0.71959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iterations</td>\n",
       "      <td>3732.00000</td>\n",
       "      <td>1692.00000</td>\n",
       "      <td>1540.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>error</td>\n",
       "      <td>0.03502</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>objective function</td>\n",
       "      <td>1825.34802</td>\n",
       "      <td>858.74560</td>\n",
       "      <td>858.74564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>last iteration's tol</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Coordinate Descent  Mini-Batch Descent  \\\n",
       "time                             0.65003             0.56995   \n",
       "iterations                    3732.00000          1692.00000   \n",
       "error                            0.03502             0.00000   \n",
       "objective function            1825.34802           858.74560   \n",
       "last iteration's tol             0.00009             0.00009   \n",
       "\n",
       "                      Momentum-Batch Descent  \n",
       "time                                 0.71959  \n",
       "iterations                        1540.00000  \n",
       "error                                0.00000  \n",
       "objective function                 858.74564  \n",
       "last iteration's tol                 0.00009  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_d = {'Coordinate Descent': coord_grad_sum, 'Mini-Batch Descent': mini_batch_sum, 'Momentum-Batch Descent': momentum_batch_sum }\n",
    "pd.DataFrame.from_dict(summary_d).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the  mini-batch  is the fastest of these, and the one that requires the lowest iterations is the mini-batch with momentum. Both the Mini-Batch without and with momentum approach very much to the objective function value obtained with the analytical solution. \n",
    "\n",
    "The Mini-Batch Descent with batch-size = 1 becomes simply the Coordinate Descent, and when the batch size becomes equal to the size of the parameters, this is just the normal gradient descent. \n",
    "\n",
    "We can see that the momentum efficiently helps the algorithm to get faster to the solution since it takes into consideration the last iteration's result. But in the contrary, we must now take into consideration the parameter $\\eta$ that will have an influence in the total number of iterations we will obtain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "To conclude, we have shown through the report different kinds of solvers and methods to reach an optimum value, in this case minimize the ridge regression equation in order to approximate the coefficients to their treu values. \n",
    "\n",
    "We have seen that there are some very fast methods but more computationally expensive, like the newton's method. And there are other methods which are computationally simpler but take longer time to reach a convergence value, like for example the coordinate descent method. \n",
    "\n",
    "We must always keep in mind the type of problem we are facing to choose the better solver, since if we are dealing with a small sample size, and it will not be that computationally heavy to perform, we could confortably pick the newton's method. But if we're working with big data sets, the most suitable option would be to pick the mini-batch descent method. \n",
    "\n",
    "Overall, this assignment has helped understand and comprehend the different kinds of algortihms to minimize or maximize a certain function and that they could be applied to thausends of applications in every field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
