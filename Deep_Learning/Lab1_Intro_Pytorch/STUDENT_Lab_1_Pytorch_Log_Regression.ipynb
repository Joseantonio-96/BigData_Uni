{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Logistic Regression with Pytorch\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "*Deep Learning. Master in Big Data Analytics*\n",
    "\n",
    "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "\n",
    "In this first lab of the course, you will learn basic aspects about how to use [Pytorch](https://pytorch.org/) to train a logistic regressor in a binary classification problem. \n",
    "\n",
    "Pytorch is a powerful library to address Machine Learning problems in general, and deep learning problems in particular, as it provides with automatic differentiation tools. Namely, in our programs we will define functions $f_{\\mathbf{w}}(x)$ of given input data $x$ and some parameters $\\mathbf{w}$ to be optimized, and we will use Pytorch's [autograd package](https://pytorch.org/docs/stable/autograd.html) to automatically compute $\\nabla_{\\mathbf{w}} f_{\\mathbf{w}}(x)$, and then use the gradient into a Stochastic Gradient Descent (SGD) routine.\n",
    "\n",
    "But lets go step by step. First of all, we will load some database and perform some pre-processing. To do so, we will use the **[Python Data Analysis Library](https://pandas.pydata.org/)** (a.k.a. pandas). You can find in Aula Global a notebook with a gentle introduction to Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pd is simply an alias \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "%config InlineBackend.figure_format = 'retina' #High quality figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Pre-processing a real dataset\n",
    "\n",
    "The [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) contains 10 features related to breast tumors that have been diagnosed as benign or malignant. You have a description of the 10 features in the link above. In the same link you can download the data set as a CVS file. \n",
    "\n",
    "We will import it directly using the following class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam():\n",
    "    data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data',header=None)\n",
    "    data.columns=['ID', 'Clump Thickness', 'Uniformity of Cell Size',\n",
    "       'Uniformity of Cell Shape', 'Marginal Adhesion',\n",
    "       'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "       'Normal Nucleoli', 'Mitoses', 'Class']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Data is stored in a Dataframe, a particular data type implemented in Pandas'''\n",
    "\n",
    "data = load_spam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the first 10 entries of the database with the `.head()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `.shape()` method we can check how many datapoints we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d data points. Each one of dimension %d' %(data.shape[0],data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the following preprocessing steps. All of them are implemented in Pandas (it is advisory for very large databases):\n",
    "\n",
    "- Remove the 'ID' column. We won't use it anymore\n",
    "\n",
    "- Analyze missing data (Encoded in this database with a '?' by the column mode). \n",
    "\n",
    "- The tumor class is equal to 2 for bening tumors and to 4 for malignat tumors. We will replace this by 0 and 1 respectively\n",
    "\n",
    "- Create a train set and a test set\n",
    "\n",
    "- We will normalize all input variables so they all have zero mean and unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove ID colum\n",
    "\n",
    "We can use the `.drop()` method. We use the input `inplace=True` to override the existing Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the result\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About missing data\n",
    "\n",
    "Pandas considers values like `NaN` and `None` to represent missing data. The `pandas.isnull` function can be used to tell whether or not a value is missing. \n",
    "\n",
    "Let's use `apply()` across all of the columns in our DataFrame to figure out which values are missing. But first we  change the `?` missing codification by `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('?', np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = data.apply(lambda col: pd.isnull(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that only one of the columns has 16 missing entries. For simplicity, let's remove this column. Be aware that in general we will impute missing data with some representative value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Bare Nuclei',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "empty = data.apply(lambda col: pd.isnull(col))\n",
    "empty.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Create a binary class label\n",
    "\n",
    "To do so, we will create a simple function to convert the label and store the result in a new colum of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(l):\n",
    "    return (l-2.0)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the new column\n",
    "\n",
    "data['Binary Class'] = data[['Class']].apply(lambda l: convert_to_binary(l))\n",
    "\n",
    "data.drop('Class',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look to the histogram of the binary class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Binary Class'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a train and test sets\n",
    "\n",
    "We will simply split the dataset at random. 80% of the data will go to the training set, the rest to the test set. We will use [Numpy's random permutation function](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.permutation.html) and the `.iloc()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_train = 0.8\n",
    "\n",
    "data_train = int(data.shape[0]*frac_train) # Training points\n",
    "\n",
    "np.random.seed(seed=10) #To fix the random seed. So we all get the same partition\n",
    "\n",
    "mask = np.random.permutation(data.shape[0]) # Random order of data indexes\n",
    "\n",
    "train_data = data.iloc[list(mask[:data_train])].copy()\n",
    "\n",
    "test_data = data.iloc[list(mask[data_train:])].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we store the train and test data in separate Dataframes, so we can normalize them without modifying the oringinal data (We could add more columns to the original Dataframe, but this would be later on annoying to index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the train/test partition and normalization can also easily done using Numpy and the [sklearn library](https://scikit-learn.org/stable/). You will see this in the Machine Learning course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize input variables \n",
    "\n",
    "In general, it is not recommended that input variables (or features) are defined in different ranges. I.e., $x_1$ for instance takes values in the $[-1,1]$ set and $x_2$ takes values in the $[-10^6,10^6]$ range. \n",
    "\n",
    "To improve both the numerical robustness of our estimators and **enhance interpretability**, we will apply a linear normalization preprocessing stage to use as input to the linear regression features with (sample) mean 0 and (sample) variance equal to 1. \n",
    "\n",
    "Given the unnormalized feature matrix $\\mathbf{X}_{N\\times D}$, we compute the sample mean per feature column, $\\mu_j$, and the sample variance per  feature column, $\\sigma^2_j$ for $j=1,\\ldots,D$. Then, each row of the normalized feature matrix $\\overline{\\mathbf{X}}_{N\\times D}$ is obtained as follows:\n",
    "\n",
    "$$\\overline{\\mathbf{x}}^{(i)}= \\left[\\frac{x_1^{(i)}-\\mu_1}{\\sqrt{\\sigma^2_1}}, ~~ \\frac{x_1^{(i)}-\\mu_2}{\\sqrt{\\sigma^2_2}}, \\ldots, \\frac{x_1^{(D)}-\\mu_1}{\\sqrt{\\sigma^2_D}}\\right]$$\n",
    "\n",
    "**Note this does not affect to the logistic regression solution**, we are simply re-scaling the parameter vector $\\boldsymbol{w}$. Using this scaling, we can now effectively compare the effect that each variable has in the logistic regression solution: **the larger $|w_j|$ is, the more effect the $j$-th feature has in the estimation of the class**.\n",
    "\n",
    "One important aspect is that the test set is normalized **using the train set statistics (mean and variance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = train_data.mean()\n",
    "stds = train_data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,column_name in enumerate(train_data.columns[:-1]):\n",
    "    print(column_name)\n",
    "    train_data[column_name] = (train_data[column_name]-means[idx])/(stds[idx])\n",
    "    test_data[column_name] = (test_data[column_name]-means[idx])/(stds[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look to the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Tensors in Pytorch\n",
    "\n",
    "\n",
    "Note: a big part of the following material is a personal wrap-up of [Facebook's Deep Learning Course in Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188). So all credit goes for them!!\n",
    "\n",
    "PyTorch in a lot of ways behaves like  Numpy arrays. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients and another module specifically for building neural networks. \n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on *tensors*, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and manipulating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â To check what torch version you're using\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a function to manipulate torch tensors. Let's program our own logistic regressor\n",
    "\n",
    "$$f(\\mathbf{x}) = \\frac{1}{1+\\exp-(\\mathbf{w}^T\\mathbf{x}+w_0)}=\\sigma(\\cdot)$$\n",
    "\n",
    "where $\\mathbf{w}$ and $w_0$ (the bias or intercept) are initialized with Pytorch tensors containing random numbers drawn from a i.i.d. Normal Gaussian distribution. Also, to test the function, we will generate data points $\\mathbf{x}$ also as random tensors.\n",
    "\n",
    "#### First we define the sigmoid function $\\sigma(\\cdot)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we generate the parameters $\\mathbf{w},w_0$ and one input vector (feature vector) $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "feature_vector = torch.randn((1, 5))\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(feature_vector)\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we generated data we can use to get the output of our simple neural network. This is all just random for now, going forward we'll start using normal data. Going through each relevant line:\n",
    "\n",
    "`features = torch.randn((1, 5))` creates a tensor with shape `(1, 5)`, one row and five columns, that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one. \n",
    "\n",
    "`weights = torch.randn_like(features)` creates another tensor with the same shape as `features`, again containing values from a normal distribution.\n",
    "\n",
    "Finally, `bias = torch.randn((1, 1))` creates a single value from a normal distribution.\n",
    "\n",
    "PyTorch tensors can be added, multiplied, subtracted, etc, just like Numpy arrays. In general, you'll use PyTorch tensors pretty much the same way you'd use Numpy arrays.\n",
    "\n",
    "Just like in Numpy, you can check the values and dimensions of a tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_vector)\n",
    "m,n = feature_vector.shape\n",
    "print(\"The dimensions are {0},{1}\".format(m,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Using the `sigmoid()` function, calculate the output a binary logistic regressor with input feature vector `feature_vector`, weights `weights`, and bias `bias`. You can use `*` for Tensor elemenwise product and the function  [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) to sum all the elements of a tensor in a certain dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Â YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the same but using matrix multiplication with `torch.mm()`\n",
    "\n",
    "\n",
    "You can do the multiplication and sum in the same operation using a matrix multiplication. In general, you'll want to use matrix multiplications since they are more efficient and accelerated using modern libraries and high-performance computing on GPUs.  For this we can use [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) which is somewhat more complicated and supports broadcasting.\n",
    "\n",
    "First, check the dimension of `feature_vector` and `weights` using the method `.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_vector.shape)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what comes if you try to directly multiply both tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(feature_vector,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening here is our tensors aren't the correct shapes to perform a matrix multiplication. There are a few options here: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "\n",
    "* `weights.reshape(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
    "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n",
    "\n",
    "Any of the two methods will work for this. So, now we can reshape `weights` to have five rows and one column with something like `weights.view(5, 1)`.\n",
    "\n",
    "\n",
    "> **Exercise**: Calculate the output of the logistic regressor using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ouput of the LR is {0}\".format(y[0][0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy to Torch and back\n",
    "\n",
    "PyTorch has a feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **memory is shared between the Numpy array and Torch tensor**, so if you change the values in-place of one object, the other will change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place (Note the _)\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Define a logistc regression and the loss function in Pytorch\n",
    "\n",
    "### The `nn.()` module\n",
    "\n",
    "PyTorch provides a module `nn` that makes building neural networks much simpler. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should to get use to inspect [Pytorch official documentation](https://pytorch.org/docs/stable/index.html) to understand the structure and usability of methods and classes required to work with Pytorch and train neural networks.\n",
    "\n",
    "With the following code I show you how to create a Logistic Regression network whose parameters will be later on optimized for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self,dimx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.randn(dimx,1),requires_grad = True)\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.randn(1,1),requires_grad = True)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.sigmoid(torch.matmul(x,self.weights)+self.bias)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this step by step.\n",
    "\n",
    "```python\n",
    "class Network(nn.Module):\n",
    "```\n",
    "\n",
    "Here we're inheriting from `nn.Module`. Combined with `super().__init__()` this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from `nn.Module` when you're creating a class for your neural network. The name of the class itself can be anything.\n",
    "\n",
    "```python\n",
    "self.weights = nn.Parameter(torch.randn(dimx,1),requires_grad = True)\n",
    "self.bias = nn.Parameter(torch.randn(1,1),requires_grad = True)\n",
    "```\n",
    "\n",
    "The Parameter class defines a kind of Tensor that is to be considered a module parameter. Namely, we will optimize the values of these tensors. Note that we define a **random initialization** with `torch.randn()`.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "```\n",
    "\n",
    "PyTorch networks created with `nn.Module` must have a `forward` method defined. It takes in a tensor `x` and passes it through the operations you defined in the `__init__` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more compact definition of the network using pre-defined layers\n",
    "\n",
    "The `.nn` package provides us with predefined layers and operators so that the implementation of our networks is indeed much easier (and compact). The following code performs exactly as the class `LR` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_compact(nn.Module):\n",
    "    def __init__(self,dimx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output = nn.Linear(dimx,1)\n",
    "    \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we define a logistic regressor by instantiating the class `LR_compact`, we can access the weights and the bias as `self.output.weight` and `self.output.bias`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the logistic regressor using our data\n",
    "\n",
    "Lets go back to the database we just preprocessed above. Consider now the first 10 datapoints in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(train_data[:10]).astype(np.float32) # train_data is a Dataframe, can easily be converted to a numpy array\n",
    "y = x[:,-1]    # Last Column is the class\n",
    "x = x[:,:-1]   # We remove the last column from x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dimensions of x are ({0},{1})\\n\".format(x.shape[0],x.shape[1]))\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Instantiate the class `LR_compact` with the dimension of every data point in `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Print the current parameters of the classifier (weight vector and bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the logistic regressor output for the data in `x`, we make use of the class `forward` method. Do not forget to feed the function with a **torch tensor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = my_classifier.forward(torch.tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the operation that created `output`, a sigmoid operation `SigmoidBackward`. The autograd module keeps track of **all operations that affect Parameter tensors** and use such information to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. \n",
    "\n",
    "###Â Evaluating the binary cross entropy function using `nn.BCE()`\n",
    "\n",
    "Let's reduce the tensor `y` to a scalar value, the binary cross entropy. We will use the class `nn.BCE()`. See how to use the function in the [official documentation](https://pytorch.org/docs/stable/nn.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = bce(output,torch.tensor(y).view(output.shape[0],1))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the operation that created `loss`, a binary cross entropy `BinaryCrossEntropyBackward`.\n",
    "\n",
    "\n",
    "Now that we know how to calculate a loss, Torch provides a module, `autograd`, for automatically calculating the gradients of the loss w.r.t. the tensors. Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way.\n",
    "To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad = True` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`. By default, all tensors created by instantiating the class `nn.Parameter` have `requires_grad = True`.\n",
    "\n",
    "> **Exercise**: Check that both the weights and the bias of the object have this flag set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`. Turning off gradients is recommended for instance when you are just evaluating you network, i.e., when training is finished and only parameter evaluation is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `.backward()` operator\n",
    "\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`. This means that when we calculate the loss and call `loss.backward()`, the gradients of the loss function w.r.t. the parameters are calculated. These gradients are used to update the weights with gradient descent.\n",
    "\n",
    "After calling  `loss.backward()` the gradient of `loss` w.r.t. to any tensor `x` can be accessed as `x.grad`\n",
    "\n",
    "\n",
    "> **Exercise**: Print the gradient of the loss function w.r.t. the model parameters for the `LR` object (or the `LR_compact`) before and after running `loss.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Complete the following code, in which given the tensor $u$ (of size $10\\times1$), you have to evaluate the gradient of $y=u^T~u$ w.r.t. every component of $u$. Does it coincide with the analytical solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.randn(10,1,requires_grad=True)\n",
    "\n",
    "#YOUR CODE HERE (2 lines of code)\n",
    "#\n",
    "#\n",
    "\n",
    "# Turn off gradients for validation, saves memory and computations\n",
    "with torch.no_grad():\n",
    "    plt.stem(u.grad.numpy(),label='Autograd',use_line_collection=True)\n",
    "    plt.plot(2*u.detach().numpy(),'-g',label='$2x$')\n",
    "    plt.grid()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.detach()` method constructs a new view on a tensor which is declared not to need gradients. You can see a nice example in this [link](http://www.bnikolic.co.uk/blog/pytorch-detach.html). Typically, we will use detach to export values to numpy to perform model evaluation. In Pytorch, you cannot call the `.numpy()` operators in Tensor that has `require_grad=True`.\n",
    "\n",
    "An important aspects to implement Gradient Descent Loops is that if call again the `.backward()` operator, then **gradients are accumulated** in the variable `x.grad`. \n",
    "\n",
    "If we recompute `y`, we can check that gradients are accumulated by calling again `y.backward()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.matmul(u.view(1,10),u)\n",
    "y.backward()\n",
    "\n",
    "plt.stem(u.grad.numpy(),label='Autograd',use_line_collection=True)\n",
    "plt.plot(2*u.detach().numpy(),'-g',label='$2x$')\n",
    "plt.plot(4*u.detach().numpy(),'-b',label='$4x$')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **reset gradients**, Pytorch optimizers provide us with the appropiate method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Train the logistic regressor with the complete dataset\n",
    "\n",
    "When we create a network with PyTorch, all of the parameters are initialized with `requires_grad = True`. This means that when we calculate the loss and call `loss.backward()`, the gradients for the parameters are calculated. These gradients are used to update the weights with gradient descent. Below you can see an example of calculating the gradients using a backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use all data points  \n",
    "\n",
    "x = np.array(train_data).astype(np.float32)\n",
    "# Last Column is the class\n",
    "y = x[:,-1]  \n",
    "x = x[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = my_classifier.forward(torch.tensor(x))\n",
    "\n",
    "loss = bce(output,torch.tensor(y).view(output.shape[0],1))\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(my_classifier.output.weight.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an optimizer\n",
    "\n",
    "There's one last piece we need to start training, an optimizer that we'll use to update the weights with the gradients. We get these from PyTorch's [`optim` package](https://pytorch.org/docs/stable/optim.html). For example we can use stochastic gradient descent with `optim.SGD`. You can see how to define an optimizer below.\n",
    "\n",
    "With the following code, we can define the optimizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(my_classifier.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a SGD iteration, we simply call `optmizer.step()`\n",
    "\n",
    "\n",
    "By the way, `my_classifier.parameters()` returns a [generator object](https://realpython.com/introduction-to-python-generators/). Namely, a list with all the parameters that you read one by one or iterate through a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = my_classifier.parameters()\n",
    "\n",
    "print(type(params))\n",
    "#Â We print the first set of parameters (weight vector)\n",
    "print(next(params))\n",
    "# We print the second set of parameters (the bias)\n",
    "print(next(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call again the method `next()` over params, Python will yield a `StopIteration` error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a class with both the network and a training method\n",
    "\n",
    "Object Oriented Programming (OOP) in Python is a versatil and convenient method to implement our own software and create reusable sofware. Indeed, along this whole notebook, we have been using classes all the time.\n",
    "\n",
    "[Here](https://www.programiz.com/python-programming/object-oriented-programming) you can find a short introduction to OOP in Python.\n",
    "\n",
    "With the following code, we will define a class that incorporates the definition of the logistic regression network and **a method to train the parameters**. Go carefully through the code and try to understand it line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This class inherits from the LR_compact class. So it has the same atributes\n",
    "and methods, and some others that we will add. \n",
    "'''\n",
    "class LR_extended(LR_compact):\n",
    "    \n",
    "    def __init__(self,dimx,sgd_iterations=1000,lr=0.001,print_every=200):\n",
    "        \n",
    "        super().__init__(dimx)  #To initialize LR_compact!\n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.optim = optim.SGD(self.parameters(), self.lr)\n",
    "        \n",
    "        self.sgd_iterations = sgd_iterations #SGD steps\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "        self.print_every = print_every\n",
    "        \n",
    "    def train(self,x,y):\n",
    "        \n",
    "        # SGD Loop\n",
    "        \n",
    "        for iter in range(int(self.sgd_iterations)):\n",
    "        \n",
    "            self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
    "\n",
    "            out = self.forward(x)\n",
    "\n",
    "            loss = self.criterion(out,y.view(x.shape[0],1))\n",
    "\n",
    "            self.loss_during_training.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optim.step()\n",
    "        \n",
    "            if(iter % self.print_every == 0): # Every 1000 iterations\n",
    "                \n",
    "                print(\"Training loss after %d iterations: %f\" \n",
    "                      %(iter,self.loss_during_training[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets instantiate the class and train the logistic regressor. See how compact and easy is now (OOP is great!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_LR = LR_extended(x.shape[1],sgd_iterations=3000,lr=0.001)\n",
    "\n",
    "my_LR.train(torch.tensor(x),torch.tensor(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you believe that training isn't finished yet, you can simply call again the `.train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_LR.train(torch.tensor(x),torch.tensor(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Train from scratch the logistic regresson for 20.000 iterations and plot the evolution of the binary cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Print all model parameters (e.g. weights and bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model in the test set\n",
    "\n",
    "Now that we have trained our logistic regressor, it is time to compare its performance in both the training and the test sets.\n",
    "\n",
    "> **Exercise**: Compute the logistic regressor output for both the training data and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train = my_LR.forward(torch.tensor(x))\n",
    "\n",
    "#Your code here\n",
    "xtest = np.array(test_data).astype(np.float32)\n",
    "# Last Column is the class\n",
    "ytest = xtest[:,-1]   \n",
    "xtest = xtest[:,:-1]\n",
    "\n",
    "out_test = my_LR.forward(torch.tensor(xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the logistic regressor output is an estimated class probability. Assuming we put the threshold at a probability of 0.5, lets count the number of errors in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate_train = np.sum((out_train.detach().numpy()>=0.5) \n",
    "                          == y.reshape([-1,1]))/y.shape[0]\n",
    "\n",
    "error_rate_test = np.sum((out_test.detach().numpy()>=0.5) \n",
    "                         == ytest.reshape([-1,1]))/ytest.shape[0]\n",
    "\n",
    "print(error_rate_train)\n",
    "\n",
    "print(error_rate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V. Training a Logistic Regressor with mini-batch stochastic gradient descent\n",
    "\n",
    "Finally, we are going to implement an scalable version of the gradient descent training implemented above. At every iteration, instead of evaluating the gradient using all data points, a small minibatch of data will be used.\n",
    "\n",
    "To make sure that all training data points are evenly used to evaluate the gradient, we will use a short iterating function. Instead of predefining a certain number of SGD iterations, we define a certain number of **epochs**. After every epoch **all datapoints** have been used in the optimizer once. \n",
    "\n",
    "\n",
    "> **Exercise**: Complete the code for the following class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This class inherits from the LR_compact class. So it has the same atributes\n",
    "and methods, and some others that we will add. \n",
    "'''\n",
    "class LR_stochastic(LR_compact):\n",
    "    \n",
    "    def __init__(self,dimx,num_train_data,\n",
    "                 epochs=100,lr=0.001,batch_size=50,print_every=20):\n",
    "        \n",
    "        super().__init__(dimx)  #To initialize LR2!\n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.optim = optim.SGD(self.parameters(), self.lr)\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.print_every = print_every  # Print loss every \n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.num_train = num_train_data\n",
    "        \n",
    "        self.num_batchs = np.floor(self.num_train/self.batch_size)\n",
    "        \n",
    "        \n",
    "    def train(self,x,y):\n",
    "        \n",
    "        # SGD Loop\n",
    "        \n",
    "        for e in range(int(self.epochs)):\n",
    "            \n",
    "            # Random data permutation at each epoch\n",
    "            \n",
    "            idx = np.random.permutation(self.num_train)\n",
    "            \n",
    "            running_loss = 0.\n",
    "            \n",
    "            for i in range(int(self.num_batchs)):\n",
    "        \n",
    "                self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
    "            \n",
    "                # Indexes of the datapoints that enter the batch\n",
    "            \n",
    "                idx_batch = idx[i*self.batch_size:(i+1)*self.batch_size]\n",
    "\n",
    "                out = self.forward(x[idx_batch,:]) \n",
    "\n",
    "                \n",
    "                loss = #Your code here\n",
    "\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                #Your code here. Compute gradients\n",
    "                \n",
    "                \n",
    "                #Your code here. Perform one SGD step\n",
    "                self.optim.step()\n",
    "                \n",
    "            self.loss_during_training.append(running_loss/self.num_batchs)\n",
    "\n",
    "            if(e % self.print_every == 0): \n",
    "\n",
    "                print(\"Training loss after %d epochs: %f\" \n",
    "                      %(e,self.loss_during_training[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Train a logistic regressor with minibatch-stochastic gradient descent with different batch sizes (10,20,50 and 100) and plot the evolution of the loss function for all cases in the same plot. \n",
    "Also, compare with the evolution of the loss function when all data is used. Note that when all datapoints are used, every GD iteration correspond to a **full epoch**.\n",
    ">\n",
    "> Finally, observe if in all cases the parameters of the models (weights and biases) are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Add a method to the above class that evaluates the classification error rate for a given set of data points and the corresponding labels. Hint: we have done this previously in the notebook, you only  have to encapsulate it in the class as a new method.\n",
    ">\n",
    ">Recall to disable gradient computation for these computations (it saves time and operations!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VI. Regularize the model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Implement a class for regularized logistic regression, in which you penalize the $L_2$ norm of the weights (do not penalize for the bias). Use a pre-defined regularization parameter that you specify when instantiate the class.\n",
    ">\n",
    "> Recall that $L_2$ is simply the sum of the squared elements of a vector.  For a given tensor `u` you can get this \n",
    "> norm by calling `torch.norm(u)`, which returns the squared-root of the norm. The idea is that you add this norm to the loss function, with a regularization parameter $\\lambda$:\n",
    ">\n",
    "> $$\\mathcal{L}_{reg} = \\mathcal{L}_{BCE} + \\lambda ||\\mathbf{w}||^2_2$$\n",
    ">\n",
    "> Train the model with SGD and compare the train/test performance as you vary $\\lambda$. Hint: Since in this case the > unregularized LR model performs really well on both train and test datasets, there is actually no need for   regularization. So do not expect much improvement, rather the contrary. We will reuse this code in future projects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
