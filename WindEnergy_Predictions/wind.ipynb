{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana;color:black;font-size:18px;line-height: 1.4em;text-align:center;\">\n",
    "    <span style=\"color:black;font-size:25px\"><b><u>Big Data Intelligence</u> </b></span><br>\n",
    "    <span style=\"color:black\"><b>Assignment 2</b></span><br>\n",
    "    <span style=\"color:gray\"><b>Predicting wind energy production with Scikit-Learn</b></span>\n",
    "</p>\n",
    "\n",
    "<p style=\"font-family:Times;color:black;font-size:15px;line-height: 1.4em;text-align:center;\">\n",
    "    <span style=\"color:black\"><b><u>Team Members</u></b></span><br>\n",
    "    <span style=\"color:black\">Jose Antonio Jijon Vorbeck - 100438045</span><br>\n",
    "    <span style=\"color:black\">Didier Renzo Dirks - 100443386</span><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is divided into three steps. \n",
    "\n",
    "The first step consists on choosing the best model with best fitted hyper-parameters to be able to predict the energy outcome in the wind farm in Sotavento given weather variables from 25 measuring stations around Sotavento. \n",
    "\n",
    "The second stage of the project consists on selecting the best attributes for the model, since some of the variables might be redundandt or non-significant, and we would like to avoid that. We will see if it would be enough to use the variables at the measuring station only. \n",
    "\n",
    "There is also a preliminary step (step 0), in which the pre-processing steps are done, like for example deletion and imputation of random data, and standardization of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 -- Some Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful libraries\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline  \n",
    "import numpy as np              \n",
    "import pandas as pd\n",
    "import os\n",
    "from numpy.random import randint\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import optuna\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "# LightBoost\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Supressing warnings from Lightboost\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn import preprocessing, metrics, tree, neighbors\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score, KFold, PredefinedSplit\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_classif, f_regression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Scikit-Optimize \n",
    "from skopt import BayesSearchCV, forest_minimize\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
    "warnings.filterwarnings('ignore', message='WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"wind_pickle\" file contains data in a binary format called \"Pickle\". Pickle data loads faster than text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('wind_pickle.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the attributes in the dataset. Very important, the output attribute (i.e. the value to be predicted, **energy**, is the first attribute). **Steps** represents the hours in advance of the forecast. We will not use this variable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5937, 556)\n"
     ]
    }
   ],
   "source": [
    "# The dataset contains 5937 instances and 556 attributes (including \n",
    "# the outcome to be predicted)\n",
    "print(data.shape)\n",
    "#data.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets put 165049 missing values \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_NIA = 100443386\n",
    "np.random.seed(my_NIA)\n",
    "\n",
    "how_many_nas = round(data.shape[0]*data.shape[1]*0.05)\n",
    "print('Lets put '+str(how_many_nas)+' missing values \\n')\n",
    "x_locations = randint(0, data.shape[0],size=how_many_nas)\n",
    "y_locations = randint(6, data.shape[1]-1,size=how_many_nas)\n",
    "\n",
    "for i in range(len(x_locations)):\n",
    "    data.iat[x_locations[i], y_locations[i]] = np.nan\n",
    "\n",
    "data.to_pickle('wind_pickle_with_nan.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, the file wind_pickle_with_nan should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will impute the missing values, standardize the data and drop the columns that are of no use for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X matrix: (5937, 550)\n",
      "Size of y vector: (5937,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('wind_pickle_with_nan.pickle')\n",
    "data = df.drop(columns=['steps', 'month', 'day', 'hour', 'year']).values\n",
    "\n",
    "y = data[:,0] \n",
    "\n",
    "# Impute missing data\n",
    "imp = SimpleImputer()\n",
    "X = imp.fit_transform(data[:,1:])\n",
    "\n",
    "# Standardize data\n",
    "scaler = preprocessing.StandardScaler().fit(X) \n",
    "X = scaler.transform(X)\n",
    "\n",
    "print(f'Shape of X matrix: {np.shape(X)}')\n",
    "print(f'Size of y vector: {np.shape(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into Train, Validation and Test, each gets 2/6 of the data, (more or less 2 years of data for each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into partitions per 2 years\n",
    "trainlen = len(df[df['year'] <= 2006])\n",
    "vallen = len(df[(df['year'] > 2006) & (df['year'] <= 2008)])\n",
    "testlen = len(df[df['year'] > 2008])\n",
    "\n",
    "X_train, y_train = X[:trainlen, :], y[:trainlen]\n",
    "X_val, y_val = X[trainlen: trainlen + vallen, :], y[trainlen: trainlen+vallen]\n",
    "X_trainval, y_trainval = X[: trainlen + vallen, :], y[: trainlen + vallen]\n",
    "X_test, y_test = X[trainlen + vallen:, :], y[trainlen + vallen:]\n",
    "\n",
    "for i in [\"X_train\", \"y_train\", \"X_val\", \"y_val\", \"X_test\", \"y_test\", \"X_trainval\", \"y_trainval\"]:\n",
    "    print(f'Shape of {i}: {np.shape(eval(i))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MODEL SELECTION AND HYPER-PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will work with 3 different models: \n",
    "* KNN\n",
    "* Random Forest\n",
    "* Gradiant Boosting\n",
    "    - XGBoost\n",
    "    - LightBoost\n",
    "\n",
    "We will run different hyper-parameter tunning optiization methods to get the best model. \n",
    "\n",
    "**The error scores between the models will be compared at the end in a comparisson table, and a final model will be chosen from that comparisson.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *a. KNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default hiper-parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228.4554818037975\n",
      "322.802831408776\n"
     ]
    }
   ],
   "source": [
    "# K --> no arguments - default parameters\n",
    "knn = neighbors.KNeighborsRegressor().fit(X_train, y_train)\n",
    "print(metrics.mean_absolute_error(y_train, knn.predict(X_train)))\n",
    "knn_default = metrics.mean_absolute_error(y_val, knn.predict(X_val))\n",
    "print(knn_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will do hyper-parameter tuning for KNN using **skopt** and **Optuna**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the validation and training partitions\n",
    "n = np.zeros(trainlen + vallen)\n",
    "n[:trainlen] = -1\n",
    "tr_val_partition = tr_val_partition = PredefinedSplit(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bayesian Search Skopt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "              error_score='raise',\n",
       "              estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=None,\n",
       "                                            n_neighbors=5, p=2,\n",
       "                                            weights='uniform'),\n",
       "              fit_params=None, iid=True, n_iter=20, n_jobs=4, n_points=1,\n",
       "              optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
       "              refit=True, return_train_score=False,\n",
       "              scoring='neg_mean_absolute_error',\n",
       "              search_spaces={'algorithm': ['auto', 'ball_tree', 'kd_tree',\n",
       "                                           'brute'],\n",
       "                             'n_neighbors': Integer(low=1, high=30, prior='uniform', transform='identity'),\n",
       "                             'p': Integer(low=1, high=2, prior='uniform', transform='identity'),\n",
       "                             'weights': ['uniform', 'distance']},\n",
       "              verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bayesian search for KNN\n",
    "param_grid = {'n_neighbors': Integer(1,30),\n",
    "              'p': Integer(1,2),\n",
    "             'weights': [\"uniform\", \"distance\"],\n",
    "             \"algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "knn_bayesian = BayesSearchCV(neighbors.KNeighborsRegressor(), \n",
    "                         param_grid,\n",
    "                         scoring='neg_mean_absolute_error',\n",
    "                         cv=tr_val_partition, \n",
    "                         n_jobs=4, n_iter=20\n",
    "                        )\n",
    "\n",
    "knn_bayesian.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('algorithm', 'brute'),\n",
       "             ('n_neighbors', 15),\n",
       "             ('p', 1),\n",
       "             ('weights', 'distance'),\n",
       "             ('best score', 313.0016595)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_bayes_sum = knn_bayesian.best_params_.copy()\n",
    "knn_bayes_sum['best score'] = round(-knn_bayesian.best_score_,7)\n",
    "knn_bayes_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    knn_opt = neighbors.KNeighborsRegressor(n_neighbors=trial.suggest_int(\"n_neighbors\", 1, 30), \n",
    "                                            p=trial.suggest_int(\"p\", 1, 2),\n",
    "                                            weights=trial.suggest_categorical('weights',[\"distance\", \"uniform\"]),\n",
    "                                            algorithm = trial.suggest_categorical('algorithm',['auto', 'ball_tree', 'kd_tree', 'brute']))\n",
    "    \n",
    "    scores = -cross_val_score(knn_opt, X_trainval, y_trainval, \n",
    "                              scoring='neg_mean_absolute_error', \n",
    "                              n_jobs=1,\n",
    "                              cv=tr_val_partition)\n",
    "    return scores.mean() \n",
    "\n",
    "np.random.seed(0)\n",
    "knn_optuna = optuna.create_study(direction=\"minimize\")\n",
    "knn_optuna.optimize(objective, n_trials=20, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto',\n",
       " 'best score': 312.6089279,\n",
       " 'n_neighbors': 17,\n",
       " 'p': 1,\n",
       " 'weights': 'distance'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_optuna_sum = knn_optuna.best_params.copy()\n",
    "knn_optuna_sum['best score']= round(knn_optuna.best_value,7)\n",
    "knn_optuna_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *b. Random Forest*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train ensemble techniques (Random Forest and Gradient Boosting) without and with hyper-parameter tuning, and compare them using the test partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278.2453281755196\n"
     ]
    }
   ],
   "source": [
    "rf_default = RandomForestRegressor()\n",
    "rf_default.fit(X_train, y_train)\n",
    "rf_default_score =metrics.mean_absolute_error(y_val, rf_default.predict(X_val))\n",
    "print(rf_default_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian (Skopt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:   21.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:   43.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:   26.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:   25.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:   11.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "              error_score='raise',\n",
       "              estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              criterion='mse', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=...\n",
       "              optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
       "              refit=True, return_train_score=False,\n",
       "              scoring='neg_mean_absolute_error',\n",
       "              search_spaces={'bootstrap': [True, False],\n",
       "                             'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90,\n",
       "                                           100, None],\n",
       "                             'max_features': ['auto', 'sqrt'],\n",
       "                             'min_samples_leaf': [1, 2, 4],\n",
       "                             'min_samples_split': [2, 5, 10],\n",
       "                             'n_estimators': Integer(low=1, high=125, prior='uniform', transform='identity')},\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bayesian search for Random Forest\n",
    "param_grid = {'n_estimators': Integer(1,125),\n",
    "              'max_depth': list(range(10,110,10)) + [None],\n",
    "             \"min_samples_split\": [2,5,10],\n",
    "              \"min_samples_leaf\": [1,2,4],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"max_features\" : ['auto', 'sqrt']\n",
    "             }\n",
    "\n",
    "rf_bayesian = BayesSearchCV(RandomForestRegressor(), \n",
    "                         param_grid,\n",
    "                         scoring='neg_mean_absolute_error',\n",
    "                         cv=tr_val_partition, \n",
    "                         n_jobs=4, n_iter=20, verbose=1\n",
    "                        )\n",
    "\n",
    "rf_bayesian.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bootstrap', True),\n",
       "             ('max_depth', 100),\n",
       "             ('max_features', 'auto'),\n",
       "             ('min_samples_leaf', 4),\n",
       "             ('min_samples_split', 10),\n",
       "             ('n_estimators', 125),\n",
       "             ('best score', 274.8243666)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_bayes_sum = rf_bayesian.best_params_.copy()\n",
    "rf_bayes_sum['best score']= round(-rf_bayesian.best_score_,7)\n",
    "rf_bayes_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna\n",
    "def objective(trial):\n",
    "    rf_opt = RandomForestRegressor(n_estimators=trial.suggest_int(\"n_estimators\", 1, 125), \n",
    "                                            max_depth=trial.suggest_categorical(\"max_depth\", list(range(10,110,10)) + [None]),\n",
    "                                           min_samples_split=trial.suggest_categorical(\"min_samples_split\", [2,5,10]),\n",
    "                                            bootstrap=trial.suggest_categorical('bootstrap',[True, False]),\n",
    "                                           min_samples_leaf=trial.suggest_categorical(\"min_samples_leaf\", [1,2,4]),\n",
    "                                            max_features=trial.suggest_categorical('max_features',['auto', 'sqrt', 'log2']))\n",
    "    \n",
    "    scores = -cross_val_score(rf_opt, X_trainval, y_trainval, \n",
    "                              scoring='neg_mean_absolute_error', \n",
    "                              n_jobs=1,\n",
    "                              cv=tr_val_partition)\n",
    "    return scores.mean() \n",
    "\n",
    "np.random.seed(0)\n",
    "rf_optuna = optuna.create_study(direction=\"minimize\")\n",
    "rf_optuna.optimize(objective, n_trials=20, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best score': 273.9664752,\n",
       " 'bootstrap': False,\n",
       " 'max_depth': 40,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 4,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 121}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_optuna_sum = rf_optuna.best_params.copy()\n",
    "rf_optuna_sum['best score']= round(rf_optuna.best_value,7)\n",
    "rf_optuna_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *c. Gradient Boosting*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198.71078813187177\n",
      "288.50233853700377\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(X_train, y_train)\n",
    "print(metrics.mean_absolute_error(y_train, gb.predict(X_train)))\n",
    "gb_default = metrics.mean_absolute_error(y_val, gb.predict(X_val))\n",
    "print(gb_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian (SkOpt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "              error_score='raise',\n",
       "              estimator=GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0,\n",
       "                                                  criterion='friedman_mse',\n",
       "                                                  init=None, learning_rate=0.1,\n",
       "                                                  loss='ls', max_depth=3,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  m...\n",
       "                             'min_samples_leaf': Integer(low=20, high=80, prior='uniform', transform='identity'),\n",
       "                             'min_samples_split': Real(low=0.001, high=1, prior='uniform', transform='identity'),\n",
       "                             'min_weight_fraction_leaf': Real(low=1e-05, high=0.01, prior='uniform', transform='identity'),\n",
       "                             'n_estimators': Integer(low=20, high=100, prior='uniform', transform='identity'),\n",
       "                             'subsample': [0.7, 0.75, 0.8, 0.85, 0.9, 0.95]},\n",
       "              verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"learning_rate\": Real(low=0.05, high=0.2),\n",
    "              'n_estimators': Integer(20,100),\n",
    "              'subsample':[0.7,0.75,0.8,0.85,0.9,0.95],\n",
    "              'max_depth': Integer(1,15),\n",
    "              \"min_samples_split\": Real(low=0.001, high=1),\n",
    "              \"min_samples_leaf\": Integer(20,80),\n",
    "              \"min_weight_fraction_leaf\": Real(low=0.00001, high=0.01),\n",
    "              \"max_features\":list(range(7,20,2))\n",
    "             }\n",
    "\n",
    "gb_bayesian = BayesSearchCV(GradientBoostingRegressor(warm_start=True), \n",
    "                         param_grid,\n",
    "                         scoring='neg_mean_absolute_error',\n",
    "                         cv=tr_val_partition, \n",
    "                         n_jobs=4, n_iter=20\n",
    "                        )\n",
    "\n",
    "gb_bayesian.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('learning_rate', 0.1725544826411087),\n",
       "             ('max_depth', 10),\n",
       "             ('max_features', 15),\n",
       "             ('min_samples_leaf', 47),\n",
       "             ('min_samples_split', 0.12761456949858813),\n",
       "             ('min_weight_fraction_leaf', 0.00738931182474084),\n",
       "             ('n_estimators', 38),\n",
       "             ('subsample', 0.8),\n",
       "             ('best score', 275.7296622)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_bayes_sum = gb_bayesian.best_params_.copy()\n",
    "gb_bayes_sum['best score']= round(-gb_bayesian.best_score_,7)\n",
    "gb_bayes_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "                \n",
    "    gb_opt = GradientBoostingRegressor(learning_rate = trial.suggest_float(\"learning_rate\",0.05,0.2),\n",
    "                                      n_estimators = trial.suggest_int('n_estimators',10,100),\n",
    "                                      subsample = trial.suggest_categorical(\"subsample\", [0.6,0.7,0.75,0.8,0.85,0.9,0.95]),\n",
    "                                      max_depth = trial.suggest_int('max_depth',1,15),\n",
    "                                      min_samples_split = trial.suggest_float(\"min_samples_split\",0.0001, 1),\n",
    "                                      min_samples_leaf = trial.suggest_int(\"min_samples_leaf\",20,80),\n",
    "                                      min_weight_fraction_leaf = trial.suggest_float(\"min_weight_fraction_leaf\",0.0001,0.5),\n",
    "                                      max_features = trial.suggest_categorical(\"max_features\",[7,9,11,13,15,17,19,21]),\n",
    "                                      warm_start=True) \n",
    "        \n",
    "        \n",
    "    scores = -cross_val_score(gb_opt, X_trainval, y_trainval, \n",
    "                              scoring='neg_mean_absolute_error', \n",
    "                              n_jobs=1,\n",
    "                              cv=tr_val_partition)\n",
    "    return scores.mean() \n",
    "\n",
    "np.random.seed(0)\n",
    "gb_optuna = optuna.create_study(direction=\"minimize\")\n",
    "gb_optuna.optimize(objective, n_trials=20, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best score': 289.1198666,\n",
       " 'learning_rate': 0.17120213003447668,\n",
       " 'max_depth': 15,\n",
       " 'max_features': 9,\n",
       " 'min_samples_leaf': 21,\n",
       " 'min_samples_split': 0.39553724519367833,\n",
       " 'min_weight_fraction_leaf': 0.007686555886978863,\n",
       " 'n_estimators': 32,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_optuna_sum = gb_optuna.best_params.copy()\n",
    "gb_optuna_sum['best score']= round(gb_optuna.best_value,7)\n",
    "gb_optuna_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *d. XGBoost*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a more engineerend version of Gradient Boosting called Extreme Gredient Boosting (XGBoosting). This consists on an optimized gradient boosting algorithm that works with parallel processing, tree prunning, handles missing values and regularizations. It is said to be very good at avoiding overfitting and the bias/variance mistakes that some other models have, according to Vishal Morde [towardsdatascience.com](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default XGBoosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:55:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "201.02705192555354\n",
      "286.51768225166967\n"
     ]
    }
   ],
   "source": [
    "xgboost = xgb.XGBRegressor()\n",
    "xgboost.fit(X_train, y_train)\n",
    "print(metrics.mean_absolute_error(y_train, xgboost.predict(X_train)))\n",
    "xgboost_default = metrics.mean_absolute_error(y_val, xgboost.predict(X_val))\n",
    "print(xgboost_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian Search (SkOpt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "              error_score='raise',\n",
       "              estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     importance_type='gain', learning_rate=0.1,\n",
       "                                     max_delta_step=0, max_depth=3,\n",
       "                                     min_child_weight=1, missing=None,\n",
       "                                     n_estimators=100, n_jobs=1, nthread=None,\n",
       "                                     o...\n",
       "                             'gamma': Real(low=0, high=0.4, prior='uniform', transform='identity'),\n",
       "                             'learning_rate': Real(low=0.01, high=0.5, prior='uniform', transform='identity'),\n",
       "                             'max_depth': Integer(low=1, high=15, prior='uniform', transform='identity'),\n",
       "                             'min_child_weight': Integer(low=1, high=10, prior='uniform', transform='identity'),\n",
       "                             'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05],\n",
       "                             'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9,\n",
       "                                           0.95]},\n",
       "              verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"learning_rate\": Real(low=0.01, high=0.5),\n",
    "              'max_depth': Integer(1,15),\n",
    "              \"min_child_weight\" : Integer(1,10),\n",
    "              \"gamma\": Real(low= 0, high = 0.4),\n",
    "              \"colsample_bytree\" : Real(low = 0.3, high = 0.8),\n",
    "              \"subsample\": [0.6,0.7,0.75,0.8,0.85,0.9,0.95],\n",
    "              \"reg_alpha\": [0, 0.001, 0.005, 0.01, 0.05],\n",
    "}\n",
    "\n",
    "xgb_bayesian = BayesSearchCV(xgb.XGBRegressor(objective ='reg:squarederror'), \n",
    "                         param_grid,\n",
    "                         scoring='neg_mean_absolute_error',\n",
    "                         cv=tr_val_partition, \n",
    "                         n_jobs=4, n_iter=20\n",
    "                        )\n",
    "\n",
    "xgb_bayesian.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('colsample_bytree', 0.7433622944092604),\n",
       "             ('gamma', 0.2802307853273717),\n",
       "             ('learning_rate', 0.042359855964878734),\n",
       "             ('max_depth', 12),\n",
       "             ('min_child_weight', 8),\n",
       "             ('reg_alpha', 0.005),\n",
       "             ('subsample', 0.6),\n",
       "             ('best score', 276.3215159)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bayes_sum = xgb_bayesian.best_params_.copy()\n",
    "xgb_bayes_sum['best score']= round(-xgb_bayesian.best_score_,7)\n",
    "xgb_bayes_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "                \n",
    "    xgb_opt = xgb.XGBRegressor(learning_rate = trial.suggest_float(\"learning_rate\",0.01,1),\n",
    "                                      n_estimators = trial.suggest_int('n_estimators',1,200),\n",
    "                                      subsample = trial.suggest_categorical(\"subsample\", [0.6,0.7,0.75,0.8,0.85,0.9,0.95]),\n",
    "                                      max_depth = trial.suggest_int('max_depth',1,15),\n",
    "                                        gamma = trial.suggest_float(\"gamma\", 0, 0.4),\n",
    "                                        min_child_weight = trial.suggest_int(\"min_child_weight\",1,10),\n",
    "                                        colsample_bytree = trial.suggest_float('colsample_bytree',0.3,0.8),\n",
    "                                        reg_alpha = trial.suggest_categorical(\"reg_alpha\", [0, 0.001, 0.005, 0.01, 0.05]),\n",
    "                                        objective =trial.suggest_categorical('objective', ['reg:squarederror'])) \n",
    "        \n",
    "        \n",
    "    scores = -cross_val_score(xgb_opt, X_trainval, y_trainval, \n",
    "                              scoring='neg_mean_absolute_error', \n",
    "                              n_jobs=1,\n",
    "                              cv=tr_val_partition)\n",
    "    return scores.mean() \n",
    "\n",
    "np.random.seed(0)\n",
    "xgb_optuna = optuna.create_study(direction=\"minimize\")\n",
    "xgb_optuna.optimize(objective, n_trials=20, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best score': 279.7236369,\n",
       " 'colsample_bytree': 0.51103483319659,\n",
       " 'gamma': 0.31223083292495046,\n",
       " 'learning_rate': 0.11589604135960821,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 2,\n",
       " 'n_estimators': 176,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'reg_alpha': 0.001,\n",
       " 'subsample': 0.85}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_optuna_sum = xgb_optuna.best_params.copy()\n",
    "xgb_optuna_sum['best score']= round(xgb_optuna.best_value,7)\n",
    "xgb_optuna_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *e. LightBoost*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light GBM is another Gradient Boositng framework based on desicion tree algorithms. It is faster in training and deliverying most optimized Hyper Parameters. I also uses less memory than its contestant XGBoost. And according to PRANJAL KHANDELWAL \"It produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. However, it can sometimes lead to overfitting which can be avoided by setting the max_depth parameter.\" [Which algorithm takes the crown: Light GBM vs XGBOOST?](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.11226523895816\n",
      "277.77222673544605\n"
     ]
    }
   ],
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "bst = lgb.train({\"verbosity\":-1, \"force_col_wise\":True}, train_data)\n",
    "\n",
    "print(metrics.mean_absolute_error(y_train, bst.predict(X_train)))\n",
    "lgb_default = metrics.mean_absolute_error(y_val, bst.predict(X_val))\n",
    "print(lgb_default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE = [\n",
    "   Real(0.01, 0.5, name='learning_rate'),\n",
    "   Integer(1, 15, name='max_depth'),\n",
    "   Integer(1,10, name=\"min_child_weight\"),\n",
    "    Categorical([0.6,0.7,0.75,0.8,0.85,0.9,0.95],name=\"subsample\"),\n",
    "    Real(low = 0.3, high = 0.8,name=\"colsample_bytree\"),\n",
    "    Categorical([0, 0.001, 0.005, 0.01, 0.05],name=\"reg_alpha\"),           \n",
    "   Integer(2, 31, name='num_leaves')]\n",
    "\n",
    "@use_named_args(SPACE)\n",
    "def objective(**params):\n",
    "    params['verbosity'] = -1\n",
    "    params['force_col_wise'] = True\n",
    "    lgb_opt = lgb.train(params, train_data)\n",
    "    return metrics.mean_absolute_error(y_val, lgb_opt.predict(X_val))\n",
    "\n",
    "\n",
    "results = forest_minimize(objective, SPACE, n_calls=20, n_jobs=4)\n",
    "lgb_bayes_best_params = {i.name:j for (i,j) in zip(SPACE, results.x)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best score': 274.2091955,\n",
       " 'colsample_bytree': 0.3167791889225895,\n",
       " 'learning_rate': 0.033753121991024745,\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 9,\n",
       " 'num_leaves': 20,\n",
       " 'reg_alpha': 0.005,\n",
       " 'subsample': 0.95}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_bayes_sum = lgb_bayes_best_params.copy()\n",
    "lgb_bayes_sum['best score']= round(results.fun,7)\n",
    "lgb_bayes_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "                \n",
    "    param = {\"learning_rate\": trial.suggest_float(\"learning_rate\",0.01,1),\n",
    "             \"num_iterations\" : trial.suggest_int('num_iterations',1,200),\n",
    "             \"subsample\" : trial.suggest_categorical(\"subsample\", [0.6,0.7,0.75,0.8,0.85,0.9,0.95]),\n",
    "             \"max_depth\" : trial.suggest_int('max_depth',1,20),\n",
    "             \"min_child_weight\" : trial.suggest_int(\"min_child_weight\",1,10),\n",
    "             \"colsample_bytree\" : trial.suggest_float('colsample_bytree',0.3,0.8),\n",
    "             \"reg_alpha\" : trial.suggest_categorical(\"reg_alpha\", [0, 0.001, 0.005, 0.01, 0.05]),\n",
    "             \"num_leaves\" : trial.suggest_int('num_leaves',2,31),\n",
    "             \"verbosity\":-1,\n",
    "             \"force_col_wise\":True\n",
    "            }\n",
    "    \n",
    "    lgb_opt = lgb.train(param, train_data, verbose_eval=False)\n",
    "    return metrics.mean_absolute_error(y_val, lgb_opt.predict(X_val))\n",
    "\n",
    "np.random.seed(0)\n",
    "lgb_optuna = optuna.create_study(direction=\"minimize\")\n",
    "lgb_optuna.optimize(objective, n_trials=20, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best score': 272.2901196,\n",
       " 'colsample_bytree': 0.47381037951518423,\n",
       " 'learning_rate': 0.043075923514380506,\n",
       " 'max_depth': 15,\n",
       " 'min_child_weight': 5,\n",
       " 'num_iterations': 159,\n",
       " 'num_leaves': 30,\n",
       " 'reg_alpha': 0.05,\n",
       " 'subsample': 0.95}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_optuna_sum = lgb_optuna.best_params.copy()\n",
    "lgb_optuna_sum['best score']= round(lgb_optuna.best_value,7)\n",
    "lgb_optuna_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have developed the 5 different models with their Hyper-parameters, we are ready to make a comparisson between them. It is straight forward to make the comparisson since we have used the same validation data for all models. \n",
    "We have used 3 different methods for HP for each model. \n",
    "* Default Parameters\n",
    "* Bayesian Search (SkOpt)\n",
    "* Optuna Optimization\n",
    "\n",
    "And we have 5 different models (KNN, Random Forest, Gradient Boosting, Extreme Gradient Boosting and Light Boosting). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Grad. Boosting</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>LightBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Default HP</th>\n",
       "      <td>322.803</td>\n",
       "      <td>278.245</td>\n",
       "      <td>288.502</td>\n",
       "      <td>286.518</td>\n",
       "      <td>277.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkOpt</th>\n",
       "      <td>313.002</td>\n",
       "      <td>274.824</td>\n",
       "      <td>275.730</td>\n",
       "      <td>276.322</td>\n",
       "      <td>274.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optuna</th>\n",
       "      <td>312.609</td>\n",
       "      <td>273.966</td>\n",
       "      <td>289.120</td>\n",
       "      <td>279.724</td>\n",
       "      <td>272.290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean Absolute Error      KNN  Random Forest  ...  XGBoost  LightBoost\n",
       "Default HP           322.803        278.245  ...  286.518     277.772\n",
       "SkOpt                313.002        274.824  ...  276.322     274.209\n",
       "Optuna               312.609        273.966  ...  279.724     272.290\n",
       "\n",
       "[3 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 models and 3 HPT methods (default, skopt, optuna)\n",
    "default = [knn_default, rf_default_score, gb_default, xgboost_default, lgb_default]\n",
    "bayes = [knn_bayes_sum['best score'], rf_bayes_sum['best score'], gb_bayes_sum['best score'], xgb_bayes_sum['best score'], lgb_bayes_sum['best score']]\n",
    "Optuna = [knn_optuna_sum['best score'], rf_optuna_sum['best score'], gb_optuna_sum['best score'], xgb_optuna_sum['best score'], lgb_optuna_sum['best score']]\n",
    "scores_sum = pd.DataFrame([default, bayes, Optuna], columns=['KNN', 'Random Forest', 'Grad. Boosting', 'XGBoost', \"LightBoost\"], index=['Default HP', 'SkOpt', 'Optuna'] ).rename_axis('Mean Absolute Error', axis=1).round(3)\n",
    "scores_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting best model from Testing Data predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the table above that the best models are nearly always obtained using SkOpt, thus we select the resulting models with the tunned HP and we use them to predict the test dataset. The best of these algorithms will be selected as the best fit for this case.\n",
    "Now we have the following 5 models to see which one delivers the best results:\n",
    "* KNN with optuna HP\n",
    "* Random Forest with SkOpt HP\n",
    "* Grad. Boosting with SkOpt HP\n",
    "* XGBoost with optuna HP\n",
    "* LightBoost with optuna HP\n",
    "\n",
    "Now we will train these models with their respective HyperParameters with the train+validation dataset, and will make predictions in the testing dataset. Then by comparisson of the results, we will select a best model, which will be our most accurate method with its corresponding Hyper Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 models with best HP \n",
    "\n",
    "#knn\n",
    "knn_final = neighbors.KNeighborsRegressor(**knn_optuna.best_params).fit(X_trainval, y_trainval)\n",
    "\n",
    "#random forest\n",
    "RF_final  = RandomForestRegressor(**rf_bayesian.best_params_).fit(X_trainval, y_trainval)\n",
    "\n",
    "#gradient boosting\n",
    "GB_final  = GradientBoostingRegressor(**gb_bayesian.best_params_).fit(X_trainval, y_trainval)\n",
    "#Extreme Gradient Boosting\n",
    "Xgb_final = xgb.XGBRegressor(**xgb_optuna.best_params).fit(X_trainval, y_trainval)\n",
    "# Light Boosting\n",
    "Lgb_final = lgb.train(lgb_optuna.best_params, lgb.Dataset(X_trainval, label=y_trainval))\n",
    "\n",
    "knn_score = metrics.mean_absolute_error(y_test, knn_final.predict(X_test))\n",
    "RF_score  = metrics.mean_absolute_error(y_test, RF_final.predict(X_test))\n",
    "GB_score  = metrics.mean_absolute_error(y_test, GB_final.predict(X_test))\n",
    "Xgb_score = metrics.mean_absolute_error(y_test, Xgb_final.predict(X_test))\n",
    "Lgb_score = metrics.mean_absolute_error(y_test, Lgb_final.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Method</th>\n",
       "      <th>Final Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>315.6850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>290.0283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grad. Boosting</th>\n",
       "      <td>294.8139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>288.5198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lightboost</th>\n",
       "      <td>283.6845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method          Final Scores\n",
       "KNN                 315.6850\n",
       "Random Forest       290.0283\n",
       "Grad. Boosting      294.8139\n",
       "XGBoost             288.5198\n",
       "Lightboost          283.6845"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models = pd.DataFrame([knn_score, RF_score, GB_score, Xgb_score, Lgb_score],columns = ['Final Scores'], index=['KNN', 'Random Forest', 'Grad. Boosting', 'XGBoost', 'Lightboost']).rename_axis('Method', axis=1).round(4)\n",
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results table, we can see that the best model to predict the energy outcome based on the weather conditions is the LightBoost, with the Hyper Parameters tunned according to the Optuna optimization method. \n",
    "\n",
    "The hyper parameters of this algorithm are presented below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.3167791889225895,\n",
       " 'learning_rate': 0.033753121991024745,\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 9,\n",
       " 'num_leaves': 20,\n",
       " 'reg_alpha': 0.005,\n",
       " 'subsample': 0.95}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_bayes_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ATTRIBUTE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will focus on answering the following questions:\n",
    "\n",
    "- Are all 550 input attributes actually necessary in order to get a good model? Is it possible to have an accurate model that uses fewer than 550 variables? How many? \n",
    "- Is it enough to use only the attributes for the actual Sotavento location? (13th location in the grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, -732.0564653103447)\n",
      "(75, -725.7027906873433)\n",
      "(83, -721.1864465371879)\n",
      "(82, -696.5284016529386)\n",
      "(80, -696.2355889214277)\n",
      "(96, -693.335745519533)\n",
      "(92, -690.1330309599442)\n",
      "(94, -689.0169342414638)\n",
      "(77, -686.7930078170858)\n",
      "(78, -686.0892835778825)\n",
      "(79, -684.9548232276951)\n",
      "(97, -683.3713705744287)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We want to rank all attributes, and the best ones will be selected later\n",
    "selector = SelectKBest(f_regression, k=\"all\")\n",
    "selector.fit(X_train, y_train)\n",
    "sorted_attributes = np.argsort(-selector.scores_)\n",
    "sorted_scores = np.sort(-selector.scores_)\n",
    "for index,element in enumerate(zip(sorted_attributes, sorted_scores)):\n",
    "    print(element)\n",
    "    if index>10: break\n",
    "        \n",
    "plt.plot(-sorted_scores)\n",
    "plt.xlabel('Attributes')\n",
    "plt.ylabel('Scoring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot and after expanding the list, we can see that there is a major break in the selector scores after the 25th attribute, and the attributes that have the highest scores correspond to the following variables in the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p59.162.10',\n",
       " 'cape.25',\n",
       " 'p59.162.8',\n",
       " 'p59.162.7',\n",
       " 'p59.162.5',\n",
       " 'p59.162.21',\n",
       " 'p59.162.17',\n",
       " 'p59.162.19',\n",
       " 'p59.162.2',\n",
       " 'p59.162.3',\n",
       " 'p59.162.4',\n",
       " 'p59.162.22',\n",
       " 'p59.162.20',\n",
       " 'p59.162.18',\n",
       " 'p59.162.13',\n",
       " 'p59.162.1',\n",
       " 'p59.162.14',\n",
       " 'p59.162.16',\n",
       " 'p59.162.6',\n",
       " 'p59.162.11',\n",
       " 'p59.162.9',\n",
       " 'p59.162.12',\n",
       " 'p59.162.15',\n",
       " 'p59.162.23',\n",
       " 'p59.162.24']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Energy_data = df.drop(columns=['steps', 'month', 'day', 'hour', 'year'])\n",
    "variables = list(Energy_data.columns) \n",
    "list( variables[i] for i in sorted_attributes[:25] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that out of this 25 attributes, 24 are named as p59.162._, the last digits corresponding to the location in the map (25 locations).\n",
    "\n",
    "By doing some research in the European Centre for Medium-Range Weather Forecasts (ECMWF) website, we have found that p59.162 corresponds to the **Vertical integral of kinetic energy** measured in J/m^2. So, we can see that having a good measurement of this components for all the positions is key to have a good model. \n",
    "\n",
    "The third highest scoring attribute, however, is named as cape.25. According to the ECMWF website, cape corresponds to **Convective available potential energy**. This is an indication of the instability (or stability) of the atmosphere and can be used to assess the potential for the development of convection, which can lead to heavy rainfall, thunderstorms and other severe weather. So by knowing the Convective available potential energy at the most continental point in the grid (position 25) we can also have a good idea of the energy that will be produced.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696.968018957346 <- Absolute error with only 25 attributes\n"
     ]
    }
   ],
   "source": [
    "# trying a decision tree with only these 25 attributes\n",
    "attr25_energy = sorted_attributes[:25]\n",
    "X_train25 = X_trainval[:,attr25_energy]\n",
    "X_test25 = X_test[:,attr25_energy]\n",
    "\n",
    "clf = tree.DecisionTreeRegressor(random_state=0)\n",
    "clf = clf.fit(X_train25, y_trainval)\n",
    "print(f'{metrics.mean_absolute_error(y_test, clf.predict(X_test25))} <- Absolute error with only 25 attributes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is impressive to see that with only 25 attributes the error is only double as before, which is of course a very big error, but nevertheless it is interesting to note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also interesting to see that models change their predictive level according to the order of the attributes, as an example, we can see below a Decision Tree with the attributes in an unordered fashion, and later in an ordered fashion, from most important to least, according to the SelectKBest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393.0906066350711\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeRegressor(random_state=0)\n",
    "clf = clf.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(metrics.mean_absolute_error(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396.8236398104265\n"
     ]
    }
   ],
   "source": [
    "X_train_new = X_trainval[:,sorted_attributes]\n",
    "X_test_new = X_test[:,sorted_attributes]\n",
    "\n",
    "clf = tree.DecisionTreeRegressor(random_state=0)\n",
    "clf = clf.fit(X_train_new, y_trainval)\n",
    "print(metrics.mean_absolute_error(y_test, clf.predict(X_test_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the difference is not great, there exist however a difference in the performance simply by changing the order of the attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Attributes as an Extra Hyper Parameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have shown that the number of attributes chosen has a big impact on the resulting score. Therefore, by selecting the number of input attributes as an extra HyperParameter, we can optimize the number of attributes to use and see if we get a similar or even better result when we reduce them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    xgb_kbest = Pipeline([\n",
    "      ('feature_selection', SelectKBest(f_regression, k = trial.suggest_int(\"feature_selection__k\",1,X_trainval.shape[1]))),\n",
    "      ('regression', xgb.XGBRegressor(**xgb_optuna.best_params))\n",
    "                        ])\n",
    "\n",
    "    scores = -cross_val_score(xgb_kbest, X_trainval, y_trainval, \n",
    "                              scoring='neg_mean_absolute_error', \n",
    "                              n_jobs=1,\n",
    "                              cv=tr_val_partition)\n",
    "    return scores.mean() \n",
    "\n",
    "np.random.seed(0)\n",
    "xgb_kbest = optuna.create_study(direction=\"minimize\")\n",
    "xgb_kbest.optimize(objective, n_trials=20, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_selection__k': 264}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_kbest.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291.8099150741541\n"
     ]
    }
   ],
   "source": [
    "Final = Pipeline([\n",
    "  ('feature_selection', SelectKBest(f_regression, k=xgb_kbest.best_params['feature_selection__k'])),\n",
    "  ('regression', xgb.XGBRegressor(**xgb_optuna.best_params))\n",
    "])\n",
    "\n",
    "Final.fit(X_train_new,y_trainval)\n",
    "print(metrics.mean_absolute_error(y_test, Final.predict(X_test_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To answer to the first question:**\n",
    "\n",
    "We have seen that some attributes are more important than others, and that the component *Vertical integral of kinetic energy* (p59.162) is key, since in the top 25 components, 24 are related to this. \n",
    "\n",
    "We have used Optuna to compute the most optimal number of attributes to select. And by means of the SelectKBest method and a pipeline to be able to select the top attributes, we have computed the mean absolute error and we see that it selects only 264 attributes while delivering an error score of 291.81. \n",
    "\n",
    "Therefore we can say that it is definetely possible to achieve a relatively good score by using only 264 attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "- *Is it enough to use only the attributes for the actual Sotavento location?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to select only the variables associated to the location 13. These variables all end with .13 in their names, when seen in the original dataset. \n",
    "Therefore we must select only these variables and run the regression model only with those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only variables associated with .13\n",
    "df13 = df[['energy']+list(filter(re.compile(\".*13$\").match, df.columns))]\n",
    "X_13 = df13.values[:,1:]\n",
    "y_13 = df13.values[:,0]\n",
    "\n",
    "X_train_13, y_train_13 = X_13[:trainlen, :], y_13[:trainlen]\n",
    "X_val_13, y_val_13 = X_13[trainlen: trainlen + vallen, :], y_13[trainlen: trainlen+vallen]\n",
    "X_trainval_13, y_trainval_13 = X_13[: trainlen + vallen, :], y_13[: trainlen + vallen]\n",
    "X_test_13, y_test_13 = X_13[trainlen + vallen:, :], y_13[trainlen + vallen:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best score': 284.8005156,\n",
       " 'colsample_bytree': 0.5984256972320868,\n",
       " 'learning_rate': 0.018263189395308226,\n",
       " 'max_depth': 18,\n",
       " 'min_child_weight': 4,\n",
       " 'num_iterations': 179,\n",
       " 'num_leaves': 20,\n",
       " 'reg_alpha': 0.01,\n",
       " 'subsample': 0.95}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the HPT method with this dataset\n",
    "traindata_13 = lgb.Dataset(X_train_13, label=y_train_13)\n",
    "def objective(trial):\n",
    "                \n",
    "    param = {\"learning_rate\": trial.suggest_float(\"learning_rate\",0.01,1),\n",
    "             \"num_iterations\" : trial.suggest_int('num_iterations',1,200),\n",
    "             \"subsample\" : trial.suggest_categorical(\"subsample\", [0.6,0.7,0.75,0.8,0.85,0.9,0.95]),\n",
    "             \"max_depth\" : trial.suggest_int('max_depth',1,20),\n",
    "             \"min_child_weight\" : trial.suggest_int(\"min_child_weight\",1,10),\n",
    "             \"colsample_bytree\" : trial.suggest_float('colsample_bytree',0.3,0.8),\n",
    "             \"reg_alpha\" : trial.suggest_categorical(\"reg_alpha\", [0, 0.001, 0.005, 0.01, 0.05]),\n",
    "             \"num_leaves\" : trial.suggest_int('num_leaves',2,31),\n",
    "             \"verbosity\":-1,\n",
    "             \"force_col_wise\":True\n",
    "            }\n",
    "    \n",
    "    lgb_opt = lgb.train(param, traindata_13, verbose_eval=False)\n",
    "    return metrics.mean_absolute_error(y_val_13, lgb_opt.predict(X_val_13))\n",
    "\n",
    "np.random.seed(0)\n",
    "lgb_optuna_13 = optuna.create_study(direction=\"minimize\")\n",
    "lgb_optuna_13.optimize(objective, n_trials=20, n_jobs=1)\n",
    "\n",
    "lgb_optuna_13_sum = lgb_optuna_13.best_params.copy()\n",
    "lgb_optuna_13_sum['best score']= round(lgb_optuna_13.best_value,7)\n",
    "lgb_optuna_13_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best score': 280.6472071,\n",
       " 'colsample_bytree': 0.6157285473753014,\n",
       " 'learning_rate': 0.05942362770821249,\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 1,\n",
       " 'num_leaves': 30,\n",
       " 'reg_alpha': 0,\n",
       " 'subsample': 0.95}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPACE = [\n",
    "   Real(0.01, 0.5, name='learning_rate'),\n",
    "   Integer(1, 15, name='max_depth'),\n",
    "   Integer(1,10, name=\"min_child_weight\"),\n",
    "    Categorical([0.6,0.7,0.75,0.8,0.85,0.9,0.95],name=\"subsample\"),\n",
    "    Real(low = 0.3, high = 0.8,name=\"colsample_bytree\"),\n",
    "    Categorical([0, 0.001, 0.005, 0.01, 0.05],name=\"reg_alpha\"),           \n",
    "   Integer(2, 31, name='num_leaves')]\n",
    "\n",
    "@use_named_args(SPACE)\n",
    "def objective(**params):\n",
    "    params['verbosity'] = -1\n",
    "    params['force_col_wise'] = True\n",
    "    lgb_opt = lgb.train(params, traindata_13)\n",
    "    return metrics.mean_absolute_error(y_val_13, lgb_opt.predict(X_val_13))\n",
    "\n",
    "\n",
    "results_13 = forest_minimize(objective, SPACE, n_calls=20, n_jobs=1)\n",
    "lgb_bayes_13_best_params = {i.name:j for (i,j) in zip(SPACE, results_13.x)}\n",
    "\n",
    "lgb_bayes_13_sum = lgb_bayes_13_best_params.copy()\n",
    "lgb_bayes_13_sum['best score']= round(results_13.fun,7)\n",
    "lgb_bayes_13_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292.89596257707177"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the parameters of the lowest error\n",
    "best_params_13 = lgb_bayes_13_best_params\n",
    "if lgb_optuna_13.best_value < results_13.fun:\n",
    "    best_params_13 = lgb_optuna_13.best_params\n",
    "    \n",
    "# fit the model and predict the results\n",
    "final_13 = lgb.train(best_params_13, lgb.Dataset(X_trainval_13, label=y_trainval_13))\n",
    "final_13_score = metrics.mean_absolute_error(y_test_13, final_13.predict(X_test_13))\n",
    "final_13_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To answer to the second question**\n",
    "\n",
    "It seems as though the outer error does not differ greatly when taking just the predictors of Sotavento instead of all of the predictors to predict the energy levels. This may indicate that taking all 500+ parameters might be too much for the model and could be actually creating sources of error. \n",
    "\n",
    "We have shown that taking only 264 attributes or even only the 13 parameters from the Sotavento location has a very similar error value than taking the whole dataset with more than 500 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "To conclude, we have shown and proved 5 different methods to predict the energy output given different parameters from 25 locations near the Sotavento wind energy plant, in Galicia. These 5 methods were the following: \n",
    "\n",
    "* KNN\n",
    "* Random Forest\n",
    "* Gradiant Boosting\n",
    "* XGBoost\n",
    "* LightBoost\n",
    "\n",
    "XGBoost and LightBoost are variations of the Gradient Boosting method. We have seen that the method that delivers the best results in the shortest time is the LighBoost method, although the XGBoost method does not fall behind by far. \n",
    "\n",
    "We have seen that the different hyperparameters obtained through the different methods (SkOpt or Optuna) perform different and are not always the same. Thus we cannot conclude whci of the two optimization methods is the best one. And we recommend always the use of both.\n",
    "\n",
    "Finally, we have seen that there is no need of using the whole 500+ available variables to make accurate predictions, we have seen in Part II of the report, that with only 264 variables we achieve a very similar error value. And even more interesting is that with only the attirbutes at location 13 (Sotavento), we obtained no significance difference than using all attributes and in significantly less time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
